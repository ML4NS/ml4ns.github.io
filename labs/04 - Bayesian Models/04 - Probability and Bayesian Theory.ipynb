{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790ce472",
   "metadata": {},
   "source": [
    "## Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "### Contributors: Antigone Fogel, Nan Fletcher-Lloyd, Anastasia Gailly De Taurines, Iona Biggart, Payam Barnaghi\n",
    "Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "\n",
    "**Spring 2026**\n",
    "\n",
    "# Lab 4: Bayesian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161ecd",
   "metadata": {},
   "source": [
    "This tutorial will focus on Bayesian theory/models using [scikit-learn](https://scikit-learn.org/stable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ac37e",
   "metadata": {},
   "source": [
    "### **Likelihood and log-likelihood**\n",
    "\n",
    "When we fit a probabilistic model, we often choose parameters that make the observed data **most likely**.\n",
    "\n",
    "For a simple example, suppose we observe a sequence of binary outcomes (e.g., 1 = disease, 0 = no disease).\n",
    "If the probability of disease is $\\theta$, then the likelihood $P(\\text{data} \\mid \\theta)$ tells us\n",
    "how plausible different values of $\\theta$ are given the data.\n",
    "\n",
    "We often use the **log-likelihood** because it turns products into sums and is numerically more stable.\n",
    "\n",
    "In the code cell below, we illustrate the idea of **likelihood** and **log-likelihood**\n",
    "using a simple binary example. We assume the data are generated by a Bernoulli process\n",
    "with an unknown probability of success $\\theta$, and examine how plausible different\n",
    "values of $\\theta$ are given the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d86188",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 0, 1, 1, 0, 1, 1, 1, 0, 1])\n",
    "n = len(data)\n",
    "k = data.sum()\n",
    "\n",
    "thetas = np.linspace(0.001, 0.999, 500)\n",
    "\n",
    "log_likelihood = k * np.log(thetas) + (n - k) * np.log(1 - thetas)\n",
    "\n",
    "theta_mle = k / n\n",
    "print(f\"MLE for theta = {theta_mle:.2f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thetas, log_likelihood)\n",
    "plt.axvline(theta_mle, linestyle=\"--\")\n",
    "plt.xlabel(\"theta\")\n",
    "plt.ylabel(\"log-likelihood (up to a constant)\")\n",
    "plt.title(\"Log-likelihood for a Bernoulli/Binomial model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d971d3",
   "metadata": {},
   "source": [
    "The curve shows how likely different values of $\\theta$ are given the observed data.\n",
    "The value of $\\theta$ that maximises the log-likelihood is the **maximum likelihood\n",
    "estimate (MLE)**. In practice, most machine learning libraries automatically perform\n",
    "this optimisation when fitting probabilistic models, including Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d554d1",
   "metadata": {},
   "source": [
    "#### **Fitting a Bayesian Classifier**\n",
    "\n",
    "Now, as you will have learnt in your lectures, **Naive Bayes** methods are a set of supervised learning classifiers that operate under the assumption that the presence of one feature in a class is independent of the presence of any other feature.\n",
    "\n",
    "There are several types of Naive Bayes models (learn more [here](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)) but for this exercise, we will focus on **[Gaussian Naive Bayes classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e67ba",
   "metadata": {},
   "source": [
    "For this, we will create our own dataset of **10 features** across **500 instances**, categorised into **two classes**: \n",
    "- **Class 0** represents a control group\n",
    "- **Class 1** represents the class of interest. \n",
    "As in real world medical problems, there are fewer instances of the class of interest.\n",
    "\n",
    "*Learn more about making your own classification datasets [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = datasets.make_classification(n_samples=500,      # Total number of samples to generate\n",
    "                                                n_features=10,      # Number of features in the dataset\n",
    "                                                n_informative=8,    # Number of features that carry useful information\n",
    "                                                n_classes=2,        # Number of classes\n",
    "                                                weights=np.array([0.65, 0.35]), # Proportion of samples in each class\n",
    "                                                flip_y=0.01,        # Fraction of samples to randomly flip (introduces noise)\n",
    "                                                class_sep=2.0,      # Separability of classes - larger=more distinct\n",
    "                                                shuffle=True,       # Shuffle dataset after generation\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de601e90",
   "metadata": {},
   "source": [
    "Now, let's take a quick look at the features and labels that were generated, in dataframe form for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c63302",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(features)\n",
    "labels = pd.DataFrame(labels)\n",
    "df = pd.concat([features, labels], axis=1)\n",
    "df.columns = [\"F\"+str(i) for i in range(10)] + ['Class']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343433f",
   "metadata": {},
   "source": [
    "For this data, we want to determine $P(\\text{label} \\mid \\text{features})$ using scikit-learn's `predict_proba`/`predict` functions.\n",
    "\n",
    "First, we must split our data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e3f0e",
   "metadata": {},
   "source": [
    "Next, we need to import and build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beeafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB(var_smoothing=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape(-1) # converts the DataFrame to a 1D array\n",
    "\n",
    "GNB.fit(x_train, y_train) # fits the model on the training data\n",
    "y_pred = GNB.predict(x_test) # predicts labels on the test data\n",
    "y_pred_probs = GNB.predict_proba(x_test) # predicted probability of each class on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61eecb",
   "metadata": {},
   "source": [
    "Notice that we didn't scale our data before fitting the Gaussian Naive Bayes model. Is scaling necessary in this case? Why or why not?\n",
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d324a1",
   "metadata": {},
   "source": [
    "Now, we want to evaluate the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7164ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred,average=None)\n",
    "recall = recall_score(y_test, y_pred,average=None)\n",
    "precision = precision_score(y_test, y_pred,average=None)\n",
    "\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1.mean())\n",
    "print('recall_avg\\t',recall.mean())\n",
    "print('precision_avg\\t',precision.mean())\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1.std())\n",
    "print('recall_sd\\t',recall.std())\n",
    "print('precision_sd\\t',precision.std())\n",
    "\n",
    "print('\\n',classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred_probs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b7703",
   "metadata": {},
   "source": [
    "#### **Probability calibration**\n",
    "\n",
    "Some classifiers can output predicted probabilities, but these probabilities\n",
    "are not always well calibrated. A well-calibrated model is one where predictions\n",
    "of 0.7 correspond to events occurring about 70% of the time.\n",
    "\n",
    "In the code cell below, we assess how well the predicted probabilities from a\n",
    "Gaussian Naive Bayes classifier are calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6109777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "y_proba = GNB.predict_proba(x_test)[:, 1] # probabilities for the positive class\n",
    "\n",
    "# Compute calibration curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test,\n",
    "    y_proba,\n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "# Brier score (lower is better)\n",
    "brier = brier_score_loss(y_test, y_proba)\n",
    "print(f\"Brier score: {brier:.3f}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure()\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, marker=\"o\", label=\"GaussianNB\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect calibration\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b64b6",
   "metadata": {},
   "source": [
    "If the model is well calibrated, the curve should lie close to the diagonal.\n",
    "Deviations from the diagonal indicate that predicted probabilities are either\n",
    "too optimistic or too conservative.\n",
    "\n",
    "The Brier score summarizes calibration quality, with lower values indicating\n",
    "better calibrated predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb75c18",
   "metadata": {},
   "source": [
    "**CHALLENGE**: In the code cell below, train a Logistic Regression classifier on the same data. Plot a calibration curve for the LR model and compare it to the one we plotted above. Which model appears to be better calibrated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bdd69",
   "metadata": {},
   "source": [
    "#### **ROC curve and AUC**\n",
    "\n",
    "So far, we have evaluated our model using a fixed decision threshold (typically 0.5).\n",
    "However, classifiers that output probabilities allow us to vary this threshold.\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve shows the trade-off between the\n",
    "true positive rate (recall) and false positive rate across all possible thresholds.\n",
    "The Area Under the Curve (AUC) summarizes this performance as a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0191c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_proba = GNB.predict_proba(x_test)[:, 1] # probabilities for the positive class\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1bbba",
   "metadata": {},
   "source": [
    "**CHALLENGE**: In the code cell below, compute predictions/probabilities at thresholds of 0.5 and 0.2. How do the evaluation metrics change? Why might different thresholds be relevant in clinical settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750151df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1815b",
   "metadata": {},
   "source": [
    "#### **Grid search**\n",
    "\n",
    "Now, these scores are not very good, particularly when it comes to identifying the class of interest (Class 1). So how can we improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d622d0",
   "metadata": {},
   "source": [
    "One method is to **tune the hyperparameters** of the model (read more about this [here](https://medium.com/analytics-vidhya/how-to-improve-naive-bayes-9fa698e14cba))\n",
    "\n",
    "For this, we first need to import the sklearn GridSearchCV function. This functions runs through all the different parameters fed into the parameter grid and produces the best combination of parameters based on a chosen scoring metric.\n",
    "\n",
    "Learn more about how to use this function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd658be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c043d",
   "metadata": {},
   "source": [
    "Next, we need to set the range of all the parameters we will feed into the parameter grid. For a Gaussian Naive Bayes classifier, the only parameter to tune is `var_smoothing`, which represents a stability calculation to widen (or smooth) the curve and therefore account for more samples that are further away from the distribution mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd54c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nb = {'var_smoothing': np.logspace(0,-9, num=1000)} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c842eeb",
   "metadata": {},
   "source": [
    "`np.logspace` returns numbers spaced evenly on a log scale, in the above example, we create a list of: 1000 samples starting from 0 and ending at -9 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1a849",
   "metadata": {},
   "source": [
    "Now, we build the GridSearchCV, using the model and parameter grid. We then fit this searching tool to the training data using a 10-fold cross-validation to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB_Grid = GridSearchCV(estimator=GaussianNB(var_smoothing=0.5), \n",
    "                        param_grid=param_grid_nb, \n",
    "                        verbose=1, \n",
    "                        cv=10, \n",
    "                        n_jobs=-1)\n",
    "GNB_Grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4887cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GNB_Grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc4ab1",
   "metadata": {},
   "source": [
    "Now we want to re-evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pred = GNB_Grid.predict(x_test)\n",
    "grid_pred_probs = GNB_Grid.predict_proba(x_test) # predicted probability of each class on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, grid_pred,average=None)\n",
    "recall = recall_score(y_test, grid_pred,average=None)\n",
    "precision = precision_score(y_test, grid_pred,average=None)\n",
    "\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1.mean())\n",
    "print('recall_avg\\t',recall.mean())\n",
    "print('precision_avg\\t',precision.mean())\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1.std())\n",
    "print('recall_sd\\t',recall.std())\n",
    "print('precision_sd\\t',precision.std())\n",
    "\n",
    "print('\\n',classification_report(y_test, grid_pred))\n",
    "print(roc_auc_score(y_test, grid_pred_probs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94211839",
   "metadata": {},
   "source": [
    "Now, these scores are much better! \n",
    "\n",
    "**CHALLENGE**: In the code cell below, plot the ROC AUC and calibration curves for your improved model. Bonus points if you can plot them in one multiplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed890df2",
   "metadata": {},
   "source": [
    "And there you have it! You've now built and tuned your first Bayesian classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd194e07",
   "metadata": {},
   "source": [
    "Now you've finished this tutorial, follow the instructions and complete the assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
