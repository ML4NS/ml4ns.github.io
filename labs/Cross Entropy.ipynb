{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "### Contributors: Payam Barnaghi, Francesca Palermo, Nan Fletcher-Lloyd, Alex Capstick, Yu Chen, Tianyu Cui, Marirena Bafaloukou, Ruxandra Mihai\n",
    "**Spring 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross Entropy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy formula takes in two distributions, $ğ‘(ğ‘¥)$, the true distribution, and $ğ‘(ğ‘¥)$, the estimated distribution, defined over the discrete variable $ğ‘¥$ and is given by\n",
    "\n",
    "ğ»(ğ‘,ğ‘)=âˆ’âˆ‘âˆ€ğ‘¥ğ‘(ğ‘¥)log(ğ‘(ğ‘¥))\n",
    "\n",
    "For a neural network, the calculation is independent of the following:\n",
    "\n",
    "- What kind of layer was used.\n",
    "\n",
    "- What kind of activation was used - although many activations will not be compatible with the calculation because their outputs are not interpretable as probabilities (i.e., their outputs are negative, greater than 1, or do not sum to 1). Softmax is often used for multiclass classification because it guarantees a well-behaved probability distribution function.\n",
    "\n",
    "For a neural network, you will usually see the equation written in a form where $ğ²$ is the ground truth vector and $ğ²Ì‚$  (or some other value taken direct from the last layer output) is the estimate. For a single example, it would look like this:\n",
    "\n",
    "ğ¿=âˆ’ğ²â‹…log(ğ²Ì‚ )\n",
    "where â‹… is the inner product.\n",
    "\n",
    "For example if the ground truth $ğ²$ gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from  estimates $ğ²Ì‚$ \n",
    "\n",
    "ğ¿=âˆ’(1Ã—ğ‘™ğ‘œğ‘”(0.1)+0Ã—log(0.5)+...)\n",
    "\n",
    "ğ¿=âˆ’ğ‘™ğ‘œğ‘”(0.1)â‰ˆ2.303\n",
    "\n",
    "An important point from comments\n",
    "\n",
    "That means, the loss would be same no matter if the predictions are [0.1,0.5,0.1,0.1,0.2] or [0.1,0.6,0.1,0.1,0.1]?\n",
    "\n",
    "Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes.\n",
    "\n",
    "You will often see this equation averaged over all examples as a cost function. It is not always strictly adhered to in descriptions, but usually a loss function is lower level and describes how a single instance or component determines an error value, whilst a cost function is higher level and describes how a complete system is evaluated for optimisation. A cost function based on multiclass log loss for data set of size ğ‘ might look like this:\n",
    "\n",
    "$ğ½=âˆ’1ğ‘(âˆ‘ğ‘–=1ğ‘ğ²ğ¢â‹…log(ğ²Ì‚ ğ¢))$\n",
    "Many implementations will require your ground truth values to be one-hot encoded (with a single true class), because that allows for some extra optimisation. However, in principle the cross entropy loss can be calculated - and optimised - when this is not the case.\n",
    "\n",
    "\n",
    "<i>source: https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation</i>\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(P,P): 1.318\n",
      "H(Q,Q): 0.755\n",
      "H(P,Q): 0.234\n",
      "H(P,Q): 6.063\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    " \n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
    " \n",
    "\n",
    "p = [0.10, 0.20, 0.30]\n",
    "q = [0.90, 0.80, 0.70]\n",
    "\n",
    "ce_pp = cross_entropy(p, p)\n",
    "print('H(P,P): %.3f' % ce_pp)\n",
    "\n",
    "ce_qq = cross_entropy(q, q)\n",
    "print('H(Q,Q): %.3f' % ce_qq)\n",
    "\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print('H(P,Q): %.3f' % ce_pq)\n",
    "\n",
    "ce_qp = cross_entropy(q, p)\n",
    "print('H(P,Q): %.3f' % ce_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || P): 0.000\n",
      "KL(Q || Q): 0.000\n",
      "KL(P || Q): -1.084\n",
      "KL(Q || P): 5.309\n"
     ]
    }
   ],
   "source": [
    "kl_pp = kl_divergence(p, p)\n",
    "print('KL(P || P): %.3f' % kl_pp)\n",
    "\n",
    "kl_qq = kl_divergence(p, p)\n",
    "print('KL(Q || Q): %.3f' % kl_qq)\n",
    "\n",
    "\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f' % kl_pq)\n",
    "\n",
    "kl_qp = kl_divergence(q, p)\n",
    "print('KL(Q || P): %.3f' % kl_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p=1.0, q=0.8] Cross Entropy: 0.223\n",
      "[p=1.0, q=0.9] Cross Entropy: 0.105\n",
      "[p=1.0, q=0.9] Cross Entropy: 0.105\n",
      "[p=1.0, q=0.6] Cross Entropy: 0.511\n",
      "[p=1.0, q=0.8] Cross Entropy: 0.223\n",
      "[p=0.0, q=0.1] Cross Entropy: 0.105\n",
      "[p=0.0, q=0.4] Cross Entropy: 0.511\n",
      "[p=0.0, q=0.2] Cross Entropy: 0.223\n",
      "[p=0.0, q=0.1] Cross Entropy: 0.105\n",
      "[p=0.0, q=0.3] Cross Entropy: 0.357\n",
      "\n",
      "Mean Cross Entropy: 0.357\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from numpy import mean\n",
    " \n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i]*log(q[i]) for i in range(len(p))])\n",
    " \n",
    "\n",
    "p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "\n",
    "sum_cross_entropy = list()\n",
    "for i in range(len(p)):\n",
    "    \n",
    "    #we need to transform each value to a distirbution \n",
    "    expected = [1.0 - p[i], p[i]]\n",
    "    predicted = [1.0 - q[i], q[i]]\n",
    "    \n",
    "    cross_ent = cross_entropy(expected, predicted)\n",
    "    print('[p=%.1f, q=%.1f] Cross Entropy: %.3f' % (p[i], q[i], cross_ent))\n",
    "    sum_cross_entropy.append(ce)\n",
    "\n",
    "mean_corss_entropy = mean(sum_cross_entropy)\n",
    "print('\\nMean Cross Entropy: %.3f' % mean_corss_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7, 0.3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
