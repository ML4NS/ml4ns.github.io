{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "### Contributors: Antigone Fogel, Nan Fletcher-Lloyd, Anastasia Gailly De Taurines, Iona Biggart, Payam Barnaghi\n",
    "Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "\n",
    "**Spring 2026**\n",
    "\n",
    "# Lab 3: Probability and Information Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability and Statistical Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will introduce you to **hypothesis testing**, a statistical method used to make decisions using experimental data. \n",
    "\n",
    "To test a hypothesis, you must first have one! Put simply, a **hypothesis** is an assumption about a population parameter that can be evaluated using sample data. **Hypothesis testing** requires two mutually exclusive statements: a null and alternate hypothesis:\n",
    "- The **null hypothesis** is a statement that assumes no effect, no difference, or no relationship exists in the population. It is the default claim to be tested\n",
    "- The **alternate hypothesis** is a statement that contradicts the null hypothesis. It suggests there is an effect, difference, or relationship in the population.\n",
    "\n",
    "Read more about the key terms of hypothesis testing and the different types and when to use them [here](https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will demonstrate how, with enough data, statistics can enable us to calculate probabilities using real-world observations. \n",
    "\n",
    "Probability provides the **theory** while statistics provides the **tools** to test that theory using **data**.\n",
    "\n",
    "In theoretical contexts, we often talk about population parameters like the \"true\" population mean or standard deviation. But, since we typically don't have access to information about the entire population, **we use sample data to estimate population parameters**. Descriptive statistics like sample mean and sample standard deviation act as *proxies* for theoretical population values.\n",
    "\n",
    "With more and more data, we can become more confident that what we calculate represents the true probability of these events occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the [iris dataset](https://scikit-learn.org/1.5/auto_examples/datasets/plot_iris_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris(as_frame=True)\n",
    "data = data.frame\n",
    "\n",
    "## Derive features and labels dfs below:\n",
    "features = \n",
    "labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one DataFrame with the features and labels\n",
    "iris = pd.concat([features, labels], axis=1)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many target values (iris types) are present in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Is the sepal length different between classes 0 and 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Exploratory analysis**\n",
    "\n",
    "Sepal length is a continuous variable. Let's begin by exploring its distribution in iris types 0 and 1 using a histogram.\n",
    "\n",
    "Learn more about histograms in seaborn [here](https://seaborn.pydata.org/generated/seaborn.histplot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell(s) below, plot a histogram showing the distributions of sepal length values for class 0 and class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHALLENGE: How else might you plot the distribution of sepal length between the two classes? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Use your histogram to roughly determine if your data is normally distributed\n",
    "\n",
    "Based on the histogram above, it looks like sepal length is approximately normally distributed in each class.\n",
    "\n",
    "**Understanding the Normal Distribution**\n",
    "\n",
    "The **normal distribution** is a foundational concept in probability and statistical theory. It describes how the probability of data points is distributed.\n",
    "- the **x-axis** represents the values of the data, and \n",
    "- the **y-axis** represents the probability of each data point, from 0 to 1.\n",
    "The highest point on the normal distribution curve represents the value with the highest probability of occuring. It corresponds to the **mean** in statistical contexts. As you move away from the mean in either direction, the probability decreases symmetrically, forming the familiar bell-shaped curve.\n",
    "\n",
    "**Comparing Two Distributions**\n",
    "\n",
    "When comparing two normal distributions:\n",
    "- **No overlap**: the two distributions likely represent two distinct datasets.\n",
    "- **Complete overlap**: the two distributions may represent the same dataset and there is no real difference in the means of the distributions.\n",
    "- **Partial overlap**: it is more difficult to determine whether or not the datasets are distinct. Further analysis is required.\n",
    "\n",
    "**Why the Normal Distribution is Important**\n",
    "\n",
    "The normal distribution is crucial in probability and statistics for two main reasons:\n",
    "1. **The Central Limit Theorem (CLT):** as we collect more data, the sample mean becomes a better estimate of the true population mean\n",
    "2. **The Three Sigma Rule:** describes how data is spread around the mean in a normal distribution\n",
    "    - **68%** of data points will fall within **1 standard deviation** of the mean \n",
    "    - **95%** of data points will fall within **2 standard deviations** of the mean \n",
    "    - **99.7%** of data points will fall within **3 standard deviations** of the mean \n",
    "\n",
    "**Applying These Concepts to the Iris Dataset**\n",
    "\n",
    "In the context of our iris dataset, we can use the Three Signma Rule to:\n",
    "- measure the spread of a specific feature (ie. sepal length) across different classes \n",
    "- quantify how likely it is that the sepal length for one class differs significantly from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create a Null and Alternate Hypothesis\n",
    "\n",
    "This will allow us to carry out hypothesis testing.\n",
    "\n",
    "**QUESTION:** Is the sepal length different between classes 0 and 1?\n",
    "- **Null Hypothesis**: Sepal length *is not* different between classes \n",
    "- **Alternate Hypothesis**: Sepal length *is* different between classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Hypothesis Testing\n",
    "\n",
    "Returning to [this article](https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce), we are reminded that there are several different statistical tests that can be conducted to test a hypothesis.\n",
    "\n",
    "**In this case,** we want to test whether the means of two independent groups are statistically different or not. Thus, we will be conducting a **two sample, two-tailed hypothesis test**.\n",
    "\n",
    "We are assuming that our data is normally distributed, but, since the sample size for each group exceeds 30, this assumption is less critial due to the CLT. Therefore, we will perform a **z-test** using each sample's **mean** and **standard deviation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the mean and standard deviation of the sepal length of each class of iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values look very similar! But let's calculate the z-test statistic before making any final decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N.B. you can learn more about using the statsmodels z-test [here](https://www.statsmodels.org/dev/generated/statsmodels.stats.weightstats.ztest.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method requires the feature columns to be in arrays. In the code cell below, create one array per class (`class_0` and `class_1`) that includes the sepal length values for each sample in that class.\n",
    "\n",
    "*HINT: consider using the `np.values` function to convert a DataFrame column to an array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n",
    "\n",
    "class_0 = \n",
    "class_1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate the test-statistic and p-value:\n",
    "\n",
    "using `ztest()`:\n",
    "- `class_0` and `class_1` are the two datasets being compared\n",
    "- `value=0` specifies the null hypothesis which assumes that there is **no difference** between the means of the two groups \n",
    "- `alternative='two-sided'` defines the type of hypothesis test. A **two-sided test** checks for differences in either direction rather than one specific direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_stat, p_val = ztest(class_0, class_1, value=0, alternative='two-sided') \n",
    "\n",
    "print(f\"test-statistic (z-value): {z_stat}\")\n",
    "print(f\"p-value: {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **p-value** helps determine whether to accept or reject the null hypothesis. It measures the probability of observing the given data (or something more extreme) if the null hypothesis were true. We call this **statistical significance** \n",
    "\n",
    "Since decisions cannot be made with 100% certainty, we set a threshold for significance, typically at 5% (0.05):\n",
    "-  If the p-value is **greater than 0.05**, we **fail to reject the null hypothesis**, indicating there is insufficient evidence to conclude a difference between the samples.\n",
    "- if the p-value is **less than 0.05**, we **reject the null hypothesis**, indicating a statistically significant difference exists between the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case,** we **reject the null hypothesis**. The sepal length is different between classes 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You've now learnt the basics of hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian Theory\n",
    "\n",
    "### **Bayes rule and the base-rate fallacy**\n",
    "\n",
    "Bayes' rule helps us update our belief about an event after seeing evidence:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P(\\text{Disease} \\mid \\text{Test+}) =\n",
    "\\frac{P(\\text{Test+} \\mid \\text{Disease})P(\\text{Disease})}\n",
    "     {P(\\text{Test+} \\mid \\text{Disease})P(\\text{Disease}) +\n",
    "      P(\\text{Test+} \\mid \\text{No Disease})P(\\text{No Disease})}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "A common mistake is to confuse **sensitivity** ($P(\\text{Test}+ \\mid \\text{Disease})$) with\n",
    "$P(\\text{Disease} \\mid \\text{Test}+)$. The latter also depends on **prevalence** (the base rate). The brief code snippet below demonstrates how these two values can, in fact, be very different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_disease_given_positive(prevalence, sensitivity, specificity):\n",
    "    # prevalence = P(Disease)\n",
    "    # sensitivity = P(Test+ | Disease)\n",
    "    # specificity = P(Test- | No Disease)\n",
    "    p_d = prevalence\n",
    "    p_not_d = 1 - p_d\n",
    "\n",
    "    p_pos_given_d = sensitivity\n",
    "    p_pos_given_not_d = 1 - specificity  # FP rate\n",
    "\n",
    "    p_pos = p_pos_given_d * p_d + p_pos_given_not_d * p_not_d\n",
    "    return (p_pos_given_d * p_d) / p_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, adjust the values for prevalence, sensitivity, and specificity, and see how $P(\\text{Disease} \\mid \\text{Test}+)$ changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these values and rerun the cell\n",
    "prevalence = 0.004\n",
    "sensitivity = 0.80\n",
    "specificity = 0.90\n",
    "\n",
    "posterior = posterior_disease_given_positive(prevalence, sensitivity, specificity)\n",
    "print(f\"P(Disease | Test+) = {posterior:.3f} ({posterior*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Density and Cumulative Distribution Functions\n",
    "\n",
    "Let's start by creating a normal distribution with mean=0 and variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0      # mean\n",
    "var = 1     # variance\n",
    "stdev = math.sqrt(var)\n",
    "\n",
    "x = np.linspace(mu - 3*stdev, mu + 3*stdev, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: In your own words, describe the variable `x` defined in the above code:\n",
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function\n",
    "\n",
    "Now, let's create a probability density function for `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = stats.norm.pdf(x, mu, stdev)\n",
    "\n",
    "plt.plot(x, pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Distribution Function\n",
    "\n",
    "Now let's create and plot a cumulative distribution functioin for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = stats.norm.cdf(x)\n",
    "\n",
    "plt.plot(x, cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Distribution\n",
    "\n",
    "Now, let's create and plot a multivariate gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, variance, and stdev for x\n",
    "mu_x = 0\n",
    "var_x = 1\n",
    "stdev_x = math.sqrt(var_x)\n",
    "\n",
    "# Mean, variance, and stdev for y\n",
    "mu_y = 0\n",
    "var_y = 1\n",
    "stdev_y = math.sqrt(var_y)\n",
    "\n",
    "# Create a grid for the multivariate gaussian distribution\n",
    "x = np.linspace(mu_x - 3*stdev_x, mu_x + 3*stdev_x, 100)\n",
    "y =np.linspace(mu_y - 3*stdev_y, mu_y + 3*stdev_y, 100)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
    "random_var = multivariate_normal([mu_x, mu_y], [[var_x, 0], [0, var_y]])\n",
    "\n",
    "#Show a 3D plot for the multivariate gaussian distribution\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, random_var.pdf(pos),cmap='viridis',linewidth=0)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_pipeline()` is a function in **scikit-learn** used to create a pipeline that sequentially applies a series of data transformations and a model. It simplifies the process of chaining preprocessing steps and machine learning algorithms together in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch dataset 1464 from OpenML ##\n",
    "X, y = fetch_openml(data_id=1464, return_X_y=True, parser='auto')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "## Define your pipeline ##\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot a confusion matrix to investigate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot(cmap='BuPu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
