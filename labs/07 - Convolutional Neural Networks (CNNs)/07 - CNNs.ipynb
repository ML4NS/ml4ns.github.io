{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFCEiai5S3I7"
      },
      "source": [
        "# Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine <br> Imperial College London\n",
        "### Contributors: Anastasia Gailly de Taurines, Nan Fletcher-Lloyd, Antigone Fogel, Iona Biggart, Payam Barnaghi\n",
        "**Spring 2026**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgF7ZK1xS3I8"
      },
      "source": [
        "# Convolutional Neural Network (CNN/ConvNet)\n",
        "\n",
        "Convolutional Neural Networks (CNNs/ConvNet) are a type of deep learning/computer vision model designed to automatically and efficiently analyse visual data (e.g., images) by assigning relevance (learnable weights and biases) to various aspects/objects in the image, and learning to distinguish between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_iVb8ZS3I8"
      },
      "source": [
        "We typically use CNNs instead of classical machine learning techniques (e.g., SVM, random forest) for tasks involving complex, high-dimensional data (like images) because CNNs are _specifically designed to handle the spatial structure and patterns in such data_.\n",
        "\n",
        "> Here \"high-dimensional\" means _a large number of features_ (not physical dimensions). Images are high-dimensional data because _each pixel can be considered a feature_. This means that a 2D image with, for example, 256x256 pixels has 65,536 features! Equally, a 3D MRI with 256x256x256 voxels (the 3D equivalent of _pixels_) has over 6.5 million features!\n",
        "\n",
        "CNNs are often better suited to handle high-dimensional data because of their:\n",
        "- **Automatic Feature Extraction**: By analysing patterns and relationships between neighbouring pixels, CNNs automatically reduce the dimensionality of high-dimensional data into self-learned features (e.g., edges, textures, shapes...). This eliminates the need for manual feature engineering, which is often required in classical techniques.\n",
        "- **Spatial Awareness**: CNNs maintain the _spatial relationships_ between pixels through operations like convolutions and pooling, making them highly effective for understanding visual content, unlike classical ML models that may treat input features as independent.\n",
        "- **Scalability to High Dimensions**: CNNs handle the large number of pixels in images without requiring explicit/manual _dimensionality reduction_. Classical techniques often struggle with the [\"curse of dimensionality\"](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning) in such cases.\n",
        "- **End-to-End Learning**: CNNs work as an _integrated pipeline_ that can handle raw image input and directly produce outputs (e.g., class labels or bounding boxes), whereas classical methods often require separate pre-processing and feature extraction steps.\n",
        "\n",
        "Additionally, CNNs have repeatedly been shown to perform exceptionally well in medical tasks such as [image classification](https://www.nature.com/articles/s41598-024-53733-6), [object detection](https://link.springer.com/article/10.1186/s12880-024-01356-8) and [segmentation](https://www.sciencedirect.com/science/article/pii/S1053811922008242?via%3Dihub#sec0005).\n",
        "\n",
        "However, CNNs have their limitations:\n",
        "- **Data Requirements**: CNNs require large amounts of labelled data to train effectively. Without sufficient data, they are prone to overfitting.\n",
        "- **Computational Cost**: Training CNNs demands high computational power, often requiring GPUs to handle the large-scale matrix operations. This can be costly and resource-intensive.\n",
        "- **'Black-Box' Nature**: CNNs lack interpretability, making it difficult to understand or explain how they make decisions ([though some improvements have been made](https://link.springer.com/chapter/10.1007/978-3-031-58181-6_11#Sec10)). This 'black-box' behaviour can be problematic in fields like healthcare, where decision transparency is crucial.\n",
        "- **Hyperparameter Sensitivity**: CNNs have numerous hyperparameters (e.g., learning rate, filter size, number of layers) that need to be fine-tuned. This process is time-consuming and requires expertise.\n",
        "\n",
        "Hence, they are typically not suited for small or tabular data, if you have computational resource constraints, or if interpretability is of key importance.\n",
        "\n",
        "Below is an example of a CNN architecture for image classification. Starting from a picture, it can pass the image through its layers and output the correct class of object present in the image (a car, in this case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvE4SE_iS3I8"
      },
      "source": [
        "<center><img src=\"./_dependents/basic_architecture.png?raw=true\" width=\"800\" /></center>\n",
        "source: https://www.mathworks.com/discovery/convolutional-neural-network.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgK0ep_FS3I8"
      },
      "source": [
        "## Using PyTorch for creating a CNN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2O-F0AS3I8"
      },
      "source": [
        "In this experiment we will use the ```PyTorch``` library for creating our own CNN model.\n",
        "\n",
        "First of all, let us import the necessary libraries with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  !git clone https://github.com/ML4NS/ml4ns.github.io\n",
        "  from distutils.dir_util import copy_tree\n",
        "  copy_tree(\"ml4ns.github.io/labs/07 - Convolutional Neural Networks (CNNs)/\", \"./\")\n",
        "  !pip install -r ml4ns.github.io/labs/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acIbPo_OS3I8"
      },
      "outputs": [],
      "source": [
        "import torch # PyTorch is an open source machine learning framework.\n",
        "import torch.nn as nn # nn package to build neural network layers\n",
        "import torch.nn.functional as F # functional interface\n",
        "import torch.utils.data as torchdata # package to load and process the data\n",
        "# The torchvision package consists of popular datasets,\n",
        "# model architectures, and common image transformations for computer vision.\n",
        "import torchvision\n",
        "from torchvision.transforms import v2 as transforms\n",
        "import numpy as np # package to work with numbers\n",
        "import os\n",
        "from pandas.core.common import flatten\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics as skmetrics\n",
        "from sklearn import calibration as skcalibration\n",
        "\n",
        "# class for holding the random state throughout the notebook.\n",
        "# this keeps results consistent\n",
        "class RandomState(object):\n",
        "    def __init__(self, random_state=None):\n",
        "        self.random_state = random_state\n",
        "    def next(self, n=1):\n",
        "        assert type(n) == int, \"Ensure n is an integer\"\n",
        "        if n == 1:\n",
        "            self.random_state,\\\n",
        "                out_state = np.random.default_rng(\n",
        "                    self.random_state\n",
        "                    ).integers(0, 1e9, size=(2,))\n",
        "        else:\n",
        "            self.random_state,\\\n",
        "                *out_state = np.random.default_rng(\n",
        "                    self.random_state\n",
        "                    ).integers(0, 1e9, size=(n+1,))\n",
        "\n",
        "        return out_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDftg4T_S3I9"
      },
      "outputs": [],
      "source": [
        "random_state = RandomState(42)\n",
        "torch.manual_seed(random_state.next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0bJcsoMS3I-"
      },
      "source": [
        "We can create a ```device``` variable to check if a GPU is available.\n",
        "If the output of device type is 'cuda', then it means that PyTorch will use a GPU for the analysis. Otherwise, CPU is used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKL9i8H6S3I-",
        "outputId": "962bf550-9d7a-4714-d5fe-29634ea1567f"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "print('GPU available: {}' .format(torch.cuda.is_available()))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWA8h7nWS3I-"
      },
      "source": [
        "## Load MRI Image Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqp3Zi-sS3I-"
      },
      "source": [
        "We will now load the  ```Alzheimer MRI Preprocessed Dataset``` dataset that we will use for the laboratory. For easier access, the dataset has already been downloaded. In the following cell, the folder containing the data gets unzipped (= uncompressed). This dataset is made of around 6400 images of 128 x 128 pixels (2D images representing an axial slice across the brain). \n",
        "\n",
        "For our analysis we have split the dataset into two classes: _Dementia_ and _Healthy_.\n",
        "\n",
        "Additional information on the dataset can be found [here](https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h95-WA_TS3I-"
      },
      "source": [
        "We need to first unzip the archive file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74JtbpDtS3I-"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# specificy the file to unpack and where to unpack it\n",
        "shutil.unpack_archive('./data/MRI_database.zip', './data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHnfSfqrS3I-"
      },
      "source": [
        "We can then start loading the data and creating the training, testing and validation datasets. This can be done easily with pytorch image folder if the files are organised in the following way:\n",
        "    \n",
        "```\n",
        "dataset_path\n",
        "    ├── class_1\n",
        "        ├── image_1\n",
        "        ...\n",
        "        ├── image_n1\n",
        "    ├── class_2\n",
        "        ├── image_1\n",
        "        ...\n",
        "    ├── class_3\n",
        "        ├── image_1\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ7oQ3l7S3I-"
      },
      "source": [
        "By default, this will load the images into `PIL.Image` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm9VcXsES3I-"
      },
      "outputs": [],
      "source": [
        "mri_dataset = torchvision.datasets.ImageFolder(\n",
        "    root='./data/MRI_database',\n",
        "    transform=None,\n",
        "    target_transform=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OERgP0XS3I-",
        "outputId": "33ed7ef1-324b-41e3-bb86-faff227c1691"
      },
      "outputs": [],
      "source": [
        "print('Number of images: ', len(mri_dataset))\n",
        "print('Number of classes: ', len(mri_dataset.classes))\n",
        "print('Classes: ', mri_dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#IF TRAINING TAKES TOO LONG/ IS TOO INTENSE FOR YOUR COMPUTER\n",
        "#Run the following lines - it will reduce the dataset size. This may reduce model performance, but at least you get to see how it all runs!\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "class CustomSubset(Subset):\n",
        "    def __init__(self, dataset, indices):\n",
        "        super(CustomSubset, self).__init__(dataset, indices)\n",
        "        self.classes = dataset.classes\n",
        "        self.class_to_idx = dataset.class_to_idx\n",
        "\n",
        "fraction = 0.1  # Use 10% of the dataset\n",
        "num_samples = int(len(mri_dataset) * fraction)\n",
        "indices = np.random.choice(len(mri_dataset), num_samples, replace=False)\n",
        "mri_dataset = CustomSubset(mri_dataset, indices)\n",
        "print('New number of images:', len(mri_dataset))\n",
        "print('New number of classes:', len(mri_dataset.classes))\n",
        "print('Classes:', mri_dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "ISLYYfhrS3I-",
        "outputId": "c53b11bc-a2e2-4c88-d802-2341ea089ac9"
      },
      "outputs": [],
      "source": [
        "print(\"Example of image with class label: \", mri_dataset[0][1])\n",
        "print(\"This image has size: \", mri_dataset[0][0].size)\n",
        "mri_dataset[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V42HTI-3S3I-"
      },
      "source": [
        "However, for our purpose we would like to load the images as _tensors_, so that we can use machine learning models (such as a CNN) and normalise the data.\n",
        "\n",
        "We do this by using the ```transforms.Compose``` function. This function allows us to chain together multiple transforms. We can then use the functions to convert the images into tensors and reshape them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-DrqOXS3I-"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToImage(), # load as tensor in pytorch\n",
        "    transforms.Grayscale(num_output_channels=1), # convert to grayscale with 1 channel, as opposed to e.g., RGB, which would require 3 channels (for red, green and blue)\n",
        "    transforms.Resize((32,32), antialias=False), # resize to 32x32 but without antialiasing (which is a form of image smoothing) - size reduction reduces the computational load of training the CNN, but may also reduce performance\n",
        "    transforms.ToDtype(torch.float32, scale=True), # convert to tensor to float32 and scale so all pixel values lie between [0,1]\n",
        "])\n",
        "\n",
        "mri_dataset = torchvision.datasets.ImageFolder(\n",
        "    root='./data/MRI_database',\n",
        "    transform=transform,\n",
        "    target_transform=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANY3rXtlS3I-"
      },
      "source": [
        "We can see nothing has changed about our dataset, except that our images are now arrays rather than PIL images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6iXDfVuS3I_",
        "outputId": "9e55ffe0-4962-425d-8f32-b2c6f14b3439"
      },
      "outputs": [],
      "source": [
        "print('Number of images: ', len(mri_dataset))\n",
        "print('Number of classes: ', len(mri_dataset.classes))\n",
        "print('Classes: ', mri_dataset.class_to_idx)\n",
        "print(\"Example of image with class label: \", mri_dataset[0][1])\n",
        "print(\"This image has size: \", mri_dataset[0][0].shape)\n",
        "mri_dataset[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8667NTGqS3I_"
      },
      "source": [
        "We can now split our dataset into our training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44SfFeh1S3I_"
      },
      "outputs": [],
      "source": [
        "test_size = int(0.2 * len(mri_dataset)) #keep 20% of whole dataset for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA4J8Ay_S3I_"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torchdata.random_split(\n",
        "    mri_dataset, [len(mri_dataset) - test_size, test_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACGZ6erjS3I_"
      },
      "source": [
        "And we can get a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEYk13nuS3I_"
      },
      "outputs": [],
      "source": [
        "val_size = int(0.2 * len(train_dataset)) #keep 20% of the training dataset for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r93g8H9fS3I_"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = torchdata.random_split(\n",
        "    mri_dataset, [len(mri_dataset) - val_size, val_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCIwtsdmS3I_",
        "outputId": "ca96acb5-3ec2-4237-9130-e66d31106b89"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"The lengths of the train, validation, and test sets are:\",\n",
        "    f\"{len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oOTxY2BS3I_"
      },
      "source": [
        "An example train image afer the resizing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "LR_ekcxES3I_",
        "outputId": "597a393d-2553-496c-eda8-38fdc3ab04d8"
      },
      "outputs": [],
      "source": [
        "random_index = np.random.default_rng(random_state.next()).integers(0, len(train_dataset))\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(3, 3))\n",
        "\n",
        "ax.imshow(train_dataset[random_index][0].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "ax.set_title(f\"Class: {train_dataset[random_index][1]}\")\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNBfdEAxS3I_"
      },
      "source": [
        "As you can see, the image seems much more pixelated than earlier - this is a result of reducing the size of the image.\n",
        "\n",
        "Recall, that our labels are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESyHGlkpS3I_",
        "outputId": "85b131df-2056-4739-be01-2f9b6dee0bb8"
      },
      "outputs": [],
      "source": [
        "print('Classes: ', mri_dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnvlEhrHS3I_"
      },
      "source": [
        "## Preparing data with DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vwvTur-S3I_"
      },
      "source": [
        "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we want to pass images in “mini batches” and reshuffle the data at every epoch to reduce model overfitting (you can find a friendly introduction to batch sizing and the advantages/disadvantages of each strategy [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)).\n",
        "\n",
        "```DataLoader``` is provided in PyTorch to automatically perform these functions.\n",
        "\n",
        "For additional information on ```DataLoader```, you can refer to the [official documentation](https://pytorch.org/docs/stable/data.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DsxsrpxS3JD"
      },
      "source": [
        "Below we will load the dataset into DataLoader with `batch_size=256`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K4tx6NzS3JD"
      },
      "outputs": [],
      "source": [
        "# first lets load it into memory to make things faster.\n",
        "# If the dataset is too large, this is not a good idea\n",
        "from dataset import MemoryDataset\n",
        "train_dataset = MemoryDataset(train_dataset, now=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvFQdA2tS3JD"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_loader = torchdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torchdata.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torchdata.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROrws63S3JE"
      },
      "source": [
        "This is the shape of a single batch now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ni4OrntS3JE",
        "outputId": "627e7fd6-5e47-4b27-a6df-d8b007df640f"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = next(iter(train_loader))\n",
        "\n",
        "print(\"The shape of our batch of images is:\", x_batch.shape)\n",
        "print(\"The shape of our batch of labels is:\", y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the image batch, \"256\" = number of image samples in this batch, the \"32\"s are the height and width of the images, and \"1\" = the number of channels (only 1 in this case as the images are in grayscale)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ysEeoVS3JE"
      },
      "source": [
        "We can use a handy torchvision function to visualise our data. This will show us a grid of images, with the corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WgRSj22S3JE"
      },
      "outputs": [],
      "source": [
        "image_grid = torchvision.utils.make_grid(x_batch, nrow=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yEsq7BSYS3JE",
        "outputId": "858b01ff-a76b-495e-f268-bad2d662f47e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(16,16))\n",
        "\n",
        "ax.imshow(image_grid[0], cmap='gray', vmin=0, vmax=1)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4lYiu-7S3JE"
      },
      "source": [
        "Additionally, we can also loop over our batch and plot the images individually with their labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2s_gTxBcS3JE",
        "outputId": "192b60ff-2bde-404f-9220-5b4ec01df690"
      },
      "outputs": [],
      "source": [
        "# {\"Dementia\": 0, \"Healthy\": 1} -> {0: \"Dementia\", 1: \"Healthy\"}\n",
        "idx_to_class = {v:k for k, v in mri_dataset.class_to_idx.items()}\n",
        "idx_to_class\n",
        "\n",
        "# plotting the images\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "gs = fig.add_gridspec(16,16)\n",
        "for n, (image, target) in enumerate(zip(x_batch, y_batch)):\n",
        "    ax = fig.add_subplot(gs[n])\n",
        "    # adding padding for space for the label\n",
        "    image = transforms.functional.pad(image, padding=(8,8,8,8))\n",
        "    ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"{idx_to_class[target.item()].split('_')[1]}\", fontsize=13, fontdict={\"color\": \"white\"}, y=0.75)\n",
        "\n",
        "plt.subplots_adjust(wspace=-0.1, hspace=-0.1, left=0, right=1.0, bottom=0, top=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX0YlN72S3JE"
      },
      "source": [
        "## Create CNN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GZuPkvNS3JE"
      },
      "source": [
        "We will now create the architecture for the CNN. In this example we will use a similar architecture to [VGG](https://viso.ai/deep-learning/vgg-very-deep-convolutional-networks/).\n",
        "\n",
        "When creating the network, we also need to specify the activation function of each layer. We will use the [Rectified Linear Units (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) in this case.\n",
        "\n",
        "If necessary, we can also introduce a regularisation layer to reduce possible overfitting. A simple way to do this can be to use dropout layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN-P4BnwS3JE"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=(1,1), bias=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=bias\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.5, bias=True):\n",
        "        super(LinearBlock, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features, out_features, bias=bias),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYhz6QOnS3JE"
      },
      "outputs": [],
      "source": [
        "in_channels = 1\n",
        "n_classes = 2\n",
        "\n",
        "def cnn_s(in_channels, n_classes):\n",
        "    return nn.Sequential(\n",
        "        ConvBlock(in_channels, 64, (3,3), stride=(1,1), padding=(1,1)),\n",
        "        ConvBlock(64, 128, (3,3), stride=(1,1), padding=(1,1)),\n",
        "        ConvBlock(128, 256, (3,3), stride=(1,1), padding=(1,1)),\n",
        "        ConvBlock(256, 256, (3,3), stride=(1,1), padding=(1,1)),\n",
        "        nn.AdaptiveAvgPool2d(output_size=(7, 7)),\n",
        "        nn.Flatten(),\n",
        "        LinearBlock(12544, 4096, dropout=0.25),\n",
        "        LinearBlock(4096, 4096, dropout=0.25),\n",
        "        LinearBlock(4096, 1000, dropout=0.25),\n",
        "        nn.Linear(1000, n_classes),\n",
        "    )\n",
        "\n",
        "\n",
        "# define the CNN architecture\n",
        "cnn = cnn_s(in_channels, n_classes)\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    cnn.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERGIR6CdS3JF"
      },
      "source": [
        "Our model is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-8psOHOS3JF",
        "outputId": "1f3c97ef-86d0-4c10-f202-2735af7ae35a"
      },
      "outputs": [],
      "source": [
        "cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI99pr65S3JF"
      },
      "source": [
        "This has the number of parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWJ8xUMVS3JF",
        "outputId": "6a810017-f18f-4b77-aa15-e04fb8cec85f"
      },
      "outputs": [],
      "source": [
        "n_params = 0\n",
        "for name, param in cnn.named_parameters():\n",
        "    n_params += param.numel()\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSWu4WMxS3JF"
      },
      "source": [
        "## Define Loss and Optimser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBP7HakOS3JF"
      },
      "source": [
        "**Loss**\n",
        "\n",
        "Deep neural networks are trained via an algorithm called gradient descent.\n",
        "It is necessary to repeatedly estimate the error for the model's current state as part of the optimisation method. To do this, we select a loss function that measures how good the model is at predicting the correct class given an image.\n",
        "\n",
        "There exist multiple and different losses based on the scope of the analysis.\n",
        "In this experiment, we are interested in classifying whether the image is of a dementia patient or a healthy individual. Our goal is to predict the probability of the sample belonging to each known class (either 0 or 1).\n",
        "\n",
        "We can then use a loss called `Cross-entropy`, which should be minimised. This loss is commonly used in classification problems.\n",
        "\n",
        "\n",
        "**Optimiser**\n",
        "\n",
        "When training the model, we need to minimise the loss function and update the weights of the model at each epoch.\n",
        "Here, the optimiser is the algorithm that modifies the weights of the neural network to reduce the loss and improve the overall accuracy.\n",
        "\n",
        "There are various types of optimiser. In our case, we will use ```Adam``` which is one of the most popular gradient descent optimisation algorithms.\n",
        "\n",
        "When setting up the optimiser, we also need to decide at which speed the model will learn. This is done via the learning rate.\n",
        "It will tell the model how fast or slow we will move towards the optimal weights\n",
        "<center><img src=\"./_dependents/learning_rate.png\" width=\"300\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzIpBe4KS3JF"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjS1R_ABS3JF"
      },
      "source": [
        "We have finally set up all the parameters and we can move on to train the model on our dataset.\n",
        "We need to set the number of iterations that we want to train our model. This number needs to be carefully decided: if it is too low, then we risk of underfitting our model, if it is too high, we risk of overfitting it.\n",
        "<center><img src=\"./_dependents/overfitting_model.png\" width=\"300\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsL4jATzS3JF"
      },
      "source": [
        "We will use a similar function as defined in the neural network lab. This can be found in the file `model_trainer.py`. We have edited this to include early stopping based on the validation loss and an option for a learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NloE_s7eS3JF"
      },
      "outputs": [],
      "source": [
        "from model_trainer import train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYoPx9f6S3JF"
      },
      "outputs": [],
      "source": [
        "n_classes = 2\n",
        "in_channels = 1\n",
        "n_epochs = 10 #recommended 10 for quick testing, 100 for better results\n",
        "\n",
        "cnn = cnn_s(in_channels, n_classes)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # cross-entropy loss for classification\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=0.001) # adam\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimiser,\n",
        "    T_max=n_epochs,\n",
        "    eta_min=0.0002,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1nTQ14S3JF",
        "outputId": "ed58aecc-19ac-4fb7-8b67-5d15200b9522"
      },
      "outputs": [],
      "source": [
        "cnn, (train_loss_dict, val_loss_dict) = train(\n",
        "    model=cnn,\n",
        "    train_loader=train_loader,\n",
        "    n_epochs=n_epochs,\n",
        "    optimiser=optimiser,\n",
        "    criterion=criterion,\n",
        "    val_loader=val_loader,\n",
        "    patience=10,\n",
        "    scheduler=scheduler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "u5sYcQJKS3JF",
        "outputId": "b2cce2c2-9053-4869-a24c-67e0415e20fb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(6,4)) # plotting area\n",
        "\n",
        "# plot training loss\n",
        "ax.plot(\n",
        "    train_loss_dict['step'], train_loss_dict['loss'],\n",
        "    label='Train'\n",
        "    )\n",
        "\n",
        "# plot validation loss\n",
        "ax.plot(\n",
        "    val_loss_dict['step'], val_loss_dict['loss'],\n",
        "    label='Val'\n",
        "    )\n",
        "\n",
        "# formatting\n",
        "ax.set_title('Training Loss')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend()\n",
        "ax.set_ylim(-0.05, 5)\n",
        "# show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U9nHSKiS3JF"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXKwoc-JS3JF"
      },
      "source": [
        "To test the model we will use the function that we have defined in the neural network lab. This can be found in the file `model_predictor.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiiLOTJES3JF"
      },
      "outputs": [],
      "source": [
        "from model_predictor import predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFVo4BQBS3JF",
        "outputId": "f3dba94d-f429-4e08-9baf-5d4a6eb01021"
      },
      "outputs": [],
      "source": [
        "outputs, targets = predict(\n",
        "    model=cnn,\n",
        "    test_loader=test_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WfSk_OiS3JG",
        "outputId": "179af841-ec93-4093-d070-bf9131b85d20"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(outputs.numpy(), axis=1)\n",
        "y_proba = F.softmax(outputs, dim=1).numpy()\n",
        "y_true = targets.numpy()\n",
        "\n",
        "print(\"The accuracy of our model is:\", skmetrics.accuracy_score(y_true, y_pred))\n",
        "print(\"The precision of our model is:\", skmetrics.precision_score(y_true, y_pred))\n",
        "print(\"The recall of our model is:\", skmetrics.recall_score(y_true, y_pred))\n",
        "print(\"The f1 score of our model is:\", skmetrics.f1_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "gBVxnWx7S3JG",
        "outputId": "d1295c68-fdef-45ab-c6ae-cc9a73f199dc"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "gs = fig.add_gridspec(1,2)\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0,0])\n",
        "skmetrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
        "ax1.set_title(\"Confusion Matrix\")\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0,1])\n",
        "skcalibration.CalibrationDisplay.from_predictions(\n",
        "    y_true, y_proba[:,1], ax=ax2\n",
        ")\n",
        "ax2.set_title(\"Calibration Curve\")\n",
        "ax2.set_aspect('equal', adjustable='box')\n",
        "\n",
        "fig.subplots_adjust(wspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzeQuZ5uS3JG"
      },
      "outputs": [],
      "source": [
        "del cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5cMEsQsS3JG"
      },
      "source": [
        "## Curious?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOToW1ddS3JG"
      },
      "source": [
        "Things to try:\n",
        "- Change the number of layers in the model (but check the input size of the first linear layer!)\n",
        "- Change the learning rate\n",
        "- Change the batch size\n",
        "- Change the number of epochs\n",
        "- Change the optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNd5tV3LS3JG"
      },
      "source": [
        "## What if I Don't Know How to Design a Model Like This?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO9umdsRS3JG"
      },
      "source": [
        "Then Pytorch has you covered! They have a few pretrained, and ready to use models that you can use for your own analysis. You can find them [here](https://pytorch.org/vision/stable/models.html). Let's try loading one of their Vision Transformer models, which are currently considered state of the art for image classification. The model called uses a Convolution to encode the input to a vector that is processed by a transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtOV_nuaS3JG"
      },
      "outputs": [],
      "source": [
        "# in pytorch it is almost too easy to get this model loaded:\n",
        "vit = torchvision.models.vit_b_16(weights=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJy2JkUQS3JG"
      },
      "source": [
        "This model is slightly bigger than the cnn we just trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7strfUIuS3JG",
        "outputId": "6a974724-a719-4aa4-e606-2a077b4de0f8"
      },
      "outputs": [],
      "source": [
        "n_params = 0\n",
        "for name, param in vit.named_parameters():\n",
        "    n_params += param.numel()\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzhZa2J7S3JG"
      },
      "source": [
        "Delete the vit to save RAM..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jH8LcaJS3JG"
      },
      "outputs": [],
      "source": [
        "del vit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6SOSFYS3JG"
      },
      "source": [
        "However, this model requires an image of at least 224 in height and width, so can't be applied to our MRI images.\n",
        "\n",
        "Instead, we will load a resnet, which is a very popular CNN architecture. This model has been pretrained on [ImageNet data](https://www.image-net.org/), which we will try and use to our advantage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijMZotlcS3JG",
        "outputId": "b654cf92-ca6c-477b-c371-6b8ab7354e01"
      },
      "outputs": [],
      "source": [
        "resnet = torchvision.models.resnet152(weights='DEFAULT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzaplP-9S3JG",
        "outputId": "e6158458-f401-4429-d3c1-b485f7edc87d"
      },
      "outputs": [],
      "source": [
        "n_params = 0\n",
        "for name, param in resnet.named_parameters():\n",
        "    n_params += param.numel()\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZJVKukcS3JG"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyf1n0-2S3JH"
      },
      "source": [
        "Since this model expects a 3 channel image, we will need to add a dimension to our images. We can either do this by repeating the image in the channel dimension, or by using a convolution. For fun, lets do the latter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5vTWiC_S3JH"
      },
      "outputs": [],
      "source": [
        "resnet = nn.Sequential(\n",
        "    nn.Conv2d(1, 3, (3,3), stride=(1,1), padding=(1,1)),\n",
        "    resnet,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xjk5-t1S3JH"
      },
      "source": [
        "Output shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCixfkM9S3JH",
        "outputId": "86d3e382-8241-40db-f86a-5efea18c4aa0"
      },
      "outputs": [],
      "source": [
        "resnet(x_batch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJ-zORKS3JH"
      },
      "source": [
        "We can see that it also outputs to 1000 classes, which is the number of classes in ImageNet. We will need to change this to 2, which is the number of classes in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrQRNVhvS3JH"
      },
      "outputs": [],
      "source": [
        "resnet = nn.Sequential(\n",
        "    resnet,\n",
        "    nn.Linear(1000, 2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzNIoPabS3JH"
      },
      "source": [
        "Output shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqbmljnpS3JH",
        "outputId": "b3ac0d70-7649-4c19-9464-726887e956fa"
      },
      "outputs": [],
      "source": [
        "resnet(x_batch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ect-wR5fS3JH"
      },
      "source": [
        "Great! Let's make a function that when called loads this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujgl3maFS3JH"
      },
      "outputs": [],
      "source": [
        "def resnet_s(in_channels, n_classes):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 3, (3,3), stride=(1,1), padding=(1,1)),\n",
        "        torchvision.models.resnet152(weights='DEFAULT'),\n",
        "        nn.Linear(1000, n_classes),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66WqVM42S3JH"
      },
      "source": [
        "Now we can train the whole model as before and make use of the pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Vd9n58S3JH"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def resnet_s_pretrained(in_channels, n_classes):\n",
        "    channel_expander = nn.Conv2d(in_channels, 3, (3,3), stride=(1,1), padding=(1,1))\n",
        "    pretrained_resnet = torchvision.models.resnet152(weights='DEFAULT')\n",
        "\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Linear(1000, 1000),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.Linear(1000, n_classes),\n",
        "    )\n",
        "\n",
        "    return nn.Sequential(\n",
        "        # ordered dict allows us to name the layers\n",
        "        OrderedDict({\n",
        "            'channel_expander': channel_expander,\n",
        "            'pretrained_resnet': pretrained_resnet,\n",
        "            'classifier': classifier,\n",
        "        })\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAS0_1w-S3JH"
      },
      "outputs": [],
      "source": [
        "n_classes = 2\n",
        "in_channels = 1\n",
        "n_epochs = 10 #recommended 10 for quick testing, 100 for better results\n",
        "\n",
        "resnet = resnet_s_pretrained(in_channels, n_classes)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # cross-entropy loss for classification\n",
        "optimiser = torch.optim.Adam(\n",
        "    resnet.parameters(),\n",
        "    lr=0.001\n",
        ") # adam\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimiser,\n",
        "    T_max=n_epochs,\n",
        "    eta_min=0.0002,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10B7KymsS3JH",
        "outputId": "2e66283b-bb6e-4955-f32e-c58d018b75e0"
      },
      "outputs": [],
      "source": [
        "n_params = 0\n",
        "for name, param in resnet.named_parameters():\n",
        "    n_params += param.numel()\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKb_tDJ6S3JH"
      },
      "source": [
        "Great! Let's try training this..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WQFFyQAS3JH"
      },
      "outputs": [],
      "source": [
        "from model_trainer import train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_IV5FU5S3JH",
        "outputId": "6f2162e3-ddae-416d-b16a-296253b680b1"
      },
      "outputs": [],
      "source": [
        "resnet, (train_loss_dict, val_loss_dict) = train(\n",
        "    model=resnet,\n",
        "    train_loader=train_loader,\n",
        "    n_epochs=n_epochs,\n",
        "    optimiser=optimiser,\n",
        "    criterion=criterion,\n",
        "    val_loader=val_loader,\n",
        "    patience=10,\n",
        "    scheduler=scheduler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "rRYfTVKTS3JH",
        "outputId": "568b857b-4d3f-438d-e0c5-a4ff885f205d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(6,4)) # plotting area\n",
        "\n",
        "# plot training loss\n",
        "ax.plot(\n",
        "    train_loss_dict['step'], train_loss_dict['loss'],\n",
        "    label='Train'\n",
        "    )\n",
        "\n",
        "# plot validation loss\n",
        "ax.plot(\n",
        "    val_loss_dict['step'], val_loss_dict['loss'],\n",
        "    label='Val'\n",
        "    )\n",
        "\n",
        "# formatting\n",
        "ax.set_title('Training Loss')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend()\n",
        "ax.set_ylim(-0.05, 1)\n",
        "# show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icj-IytaS3JI"
      },
      "outputs": [],
      "source": [
        "from model_predictor import predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FWa-CHXS3JI",
        "outputId": "c1ac5f6f-e955-4a4c-b0d3-03056af4e161"
      },
      "outputs": [],
      "source": [
        "outputs, targets = predict(\n",
        "    model=resnet,\n",
        "    test_loader=test_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NybYWLClS3JI",
        "outputId": "57f21997-0792-4c15-d0b1-cad7f49a9d8a"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(outputs.numpy(), axis=1)\n",
        "y_proba = F.softmax(outputs, dim=1).numpy()\n",
        "y_true = targets.numpy()\n",
        "\n",
        "print(\"The accuracy of our model is:\", skmetrics.accuracy_score(y_true, y_pred))\n",
        "print(\"The precision of our model is:\", skmetrics.precision_score(y_true, y_pred))\n",
        "print(\"The recall of our model is:\", skmetrics.recall_score(y_true, y_pred))\n",
        "print(\"The f1 score of our model is:\", skmetrics.f1_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "mjIAWMLiS3JI",
        "outputId": "724d419b-d19e-40b8-bc78-d4ad2c9521f1"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,4))\n",
        "gs = fig.add_gridspec(1,2)\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0,0])\n",
        "skmetrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
        "ax1.set_title(\"Confusion Matrix\")\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0,1])\n",
        "skcalibration.CalibrationDisplay.from_predictions(\n",
        "    y_true, y_proba[:,1], ax=ax2\n",
        ")\n",
        "ax2.set_title(\"Calibration Curve\")\n",
        "ax2.set_aspect('equal', adjustable='box')\n",
        "\n",
        "fig.subplots_adjust(wspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1JdbMiuS3JI"
      },
      "outputs": [],
      "source": [
        "del resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y5-fDJCS3JI"
      },
      "source": [
        "## Are Other Models Available?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF96sMk5S3JI"
      },
      "source": [
        "Yes! We can also use hugging face to explore available models (and not just vision models!): https://huggingface.co/models?pipeline_tag=image-classification&sort=trending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSa5ca5lS3JI"
      },
      "outputs": [],
      "source": [
        "from transformers import ConvNextForImageClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e9730c579c744a3fb68d495e0e123f21",
            "226394e4e2b44beba493c110e2ac8c01",
            "8ad349ba42914040a228c4c15ac5d533",
            "7d51ca4b79a94e42bbb9888fdcae67bb",
            "8b3ef248cf4b4ff4b70a08d706dcb605",
            "5122ca20e68c4040bd08d17896b63ce2",
            "3f6a9240408f477f8f39df1037e57a8f",
            "8059a3b3bb4d4641899079617b9b3cf2",
            "bc748c6a991a453ebdae48627fca0628",
            "af804eb59e12453796127fc9eac3099b",
            "9c0b4221cc464380821ae1dbe0bdafce",
            "78086835479d43dbb20bdbda708f8023",
            "bbb0b8c5cd1d4d68ace19183ad50b204",
            "6c7882a9e87a473e9410e0d8fff292c6",
            "b692163351ca45169fe1f1735c1f1528",
            "87faf8a8415541c0b719e203451e9a7f",
            "cf20124f2c5340668e4c62eaac57d4b0",
            "84a1ade0ed504e759c746b645eb8f16b",
            "38a247489e974b59bcd9d231d6553aed",
            "79a25d79ab9848edbb7487380502e67f",
            "69450c8fddca4c06a9c27a2b2d387963",
            "e315e8bbe00f4efead0f49e5e2dc148d"
          ]
        },
        "id": "gNKBDGVoS3JI",
        "outputId": "bb5fea81-b952-4069-a31d-1f1dc199a2cd"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "convnext = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbUsqir9S3JI"
      },
      "outputs": [],
      "source": [
        "del convnext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqeh7yeIS3JI"
      },
      "source": [
        "We can put these models into a `nn.Module` that will create our final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57M4fYLJS3JI"
      },
      "outputs": [],
      "source": [
        "class ConvNext(nn.Module):\n",
        "    def __init__(self, in_channels, n_classes) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        # this pretrained model requires three channels\n",
        "        self.channel_expander = nn.Conv2d(in_channels, 3, (3,3), stride=(1,1), padding=(1,1))\n",
        "        self.convnext = ConvNextForImageClassification.from_pretrained(\n",
        "           \"facebook/convnext-tiny-224\"\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(1000, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_expander(x)\n",
        "        x = self.convnext(x).logits\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSSY2X5BS3JI"
      },
      "outputs": [],
      "source": [
        "n_classes = 2\n",
        "in_channels = 1\n",
        "n_epochs = 10 #recommended 10 for quick testing, 100 for better results\n",
        "\n",
        "convnext = ConvNext(in_channels, n_classes)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # cross-entropy loss for classification\n",
        "optimiser = torch.optim.SGD(\n",
        "    convnext.parameters(),\n",
        "    lr=0.01\n",
        ") # adam didn't work well here but SGD did...\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimiser,\n",
        "    T_max=n_epochs,\n",
        "    eta_min=0.0002,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCRfbZPwS3JI",
        "outputId": "97bb7575-486a-4f83-bed9-a927da9407c6"
      },
      "outputs": [],
      "source": [
        "n_params = 0\n",
        "for name, param in convnext.named_parameters():\n",
        "    n_params += param.numel()\n",
        "print(f\"Number of parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNYYH_1tS3JI"
      },
      "outputs": [],
      "source": [
        "from model_trainer import train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_batch, y_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO9m-g7KS3JI",
        "outputId": "de3eeedd-dbf3-4d23-a61d-45e469856ebe"
      },
      "outputs": [],
      "source": [
        "convnext, (train_loss_dict, val_loss_dict) = train(\n",
        "    model=convnext,\n",
        "    train_loader=train_loader,\n",
        "    n_epochs=n_epochs,\n",
        "    optimiser=optimiser,\n",
        "    criterion=criterion,\n",
        "    val_loader=val_loader,\n",
        "    patience=10,\n",
        "    scheduler=scheduler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "DxN-6iiiS3JI",
        "outputId": "9b4ced90-9b29-4815-8f2f-670f5861ac67"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(6,4)) # plotting area\n",
        "\n",
        "# plot training loss\n",
        "ax.plot(\n",
        "    train_loss_dict['step'], train_loss_dict['loss'],\n",
        "    label='Train'\n",
        "    )\n",
        "\n",
        "# plot validation loss\n",
        "ax.plot(\n",
        "    val_loss_dict['step'], val_loss_dict['loss'],\n",
        "    label='Val'\n",
        "    )\n",
        "\n",
        "# formatting\n",
        "ax.set_title('Training Loss')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend()\n",
        "ax.set_ylim(-0.05, 1)\n",
        "# show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutXO47SS3JJ"
      },
      "outputs": [],
      "source": [
        "from model_predictor import predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iKZ8bPQS3JJ",
        "outputId": "fe50c96a-476a-45f4-9442-52095bf15aff"
      },
      "outputs": [],
      "source": [
        "outputs, targets = predict(\n",
        "    model=convnext,\n",
        "    test_loader=test_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3vSPSPdS3JJ",
        "outputId": "0898d539-fe11-4baa-bb6c-7a104f084025"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(outputs.numpy(), axis=1)\n",
        "y_proba = F.softmax(outputs, dim=1).numpy()\n",
        "y_true = targets.numpy()\n",
        "\n",
        "print(\"The accuracy of our model is:\", skmetrics.accuracy_score(y_true, y_pred))\n",
        "print(\"The precision of our model is:\", skmetrics.precision_score(y_true, y_pred))\n",
        "print(\"The recall of our model is:\", skmetrics.recall_score(y_true, y_pred))\n",
        "print(\"The f1 score of our model is:\", skmetrics.f1_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "f0pHuPxlS3JJ",
        "outputId": "7016c938-cda4-4df0-bafc-a7207dc7ef8c"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,4))\n",
        "gs = fig.add_gridspec(1,2)\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0,0])\n",
        "skmetrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
        "ax1.set_title(\"Confusion Matrix\")\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0,1])\n",
        "skcalibration.CalibrationDisplay.from_predictions(\n",
        "    y_true, y_proba[:,1], ax=ax2\n",
        ")\n",
        "ax2.set_title(\"Calibration Curve\")\n",
        "ax2.set_aspect('equal', adjustable='box')\n",
        "\n",
        "fig.subplots_adjust(wspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqhptwLES3JJ"
      },
      "outputs": [],
      "source": [
        "del convnext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, in this task the final model (which is the smallest) performs the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVBBssC3S3JJ"
      },
      "source": [
        "## Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQdykhMVS3JJ"
      },
      "source": [
        "- Documentation for the Convolutional Layer - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "- Guide and discussion on optimisers - https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0\n",
        "- CIFAR10 Tutorial - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "- CNN implementation on CIFAR10 - https://shonit2096.medium.com/cnn-on-cifar10-data-set-using-pytorch-34be87e09844"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml4ns",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "226394e4e2b44beba493c110e2ac8c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5122ca20e68c4040bd08d17896b63ce2",
            "placeholder": "​",
            "style": "IPY_MODEL_3f6a9240408f477f8f39df1037e57a8f",
            "value": "config.json: 100%"
          }
        },
        "38a247489e974b59bcd9d231d6553aed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6a9240408f477f8f39df1037e57a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5122ca20e68c4040bd08d17896b63ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69450c8fddca4c06a9c27a2b2d387963": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7882a9e87a473e9410e0d8fff292c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38a247489e974b59bcd9d231d6553aed",
            "max": 114423189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79a25d79ab9848edbb7487380502e67f",
            "value": 114423189
          }
        },
        "78086835479d43dbb20bdbda708f8023": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbb0b8c5cd1d4d68ace19183ad50b204",
              "IPY_MODEL_6c7882a9e87a473e9410e0d8fff292c6",
              "IPY_MODEL_b692163351ca45169fe1f1735c1f1528"
            ],
            "layout": "IPY_MODEL_87faf8a8415541c0b719e203451e9a7f"
          }
        },
        "79a25d79ab9848edbb7487380502e67f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d51ca4b79a94e42bbb9888fdcae67bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af804eb59e12453796127fc9eac3099b",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0b4221cc464380821ae1dbe0bdafce",
            "value": " 69.6k/69.6k [00:00&lt;00:00, 1.45MB/s]"
          }
        },
        "8059a3b3bb4d4641899079617b9b3cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84a1ade0ed504e759c746b645eb8f16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87faf8a8415541c0b719e203451e9a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad349ba42914040a228c4c15ac5d533": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8059a3b3bb4d4641899079617b9b3cf2",
            "max": 69640,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc748c6a991a453ebdae48627fca0628",
            "value": 69640
          }
        },
        "8b3ef248cf4b4ff4b70a08d706dcb605": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0b4221cc464380821ae1dbe0bdafce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af804eb59e12453796127fc9eac3099b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b692163351ca45169fe1f1735c1f1528": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69450c8fddca4c06a9c27a2b2d387963",
            "placeholder": "​",
            "style": "IPY_MODEL_e315e8bbe00f4efead0f49e5e2dc148d",
            "value": " 114M/114M [00:01&lt;00:00, 151MB/s]"
          }
        },
        "bbb0b8c5cd1d4d68ace19183ad50b204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf20124f2c5340668e4c62eaac57d4b0",
            "placeholder": "​",
            "style": "IPY_MODEL_84a1ade0ed504e759c746b645eb8f16b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "bc748c6a991a453ebdae48627fca0628": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf20124f2c5340668e4c62eaac57d4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e315e8bbe00f4efead0f49e5e2dc148d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9730c579c744a3fb68d495e0e123f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_226394e4e2b44beba493c110e2ac8c01",
              "IPY_MODEL_8ad349ba42914040a228c4c15ac5d533",
              "IPY_MODEL_7d51ca4b79a94e42bbb9888fdcae67bb"
            ],
            "layout": "IPY_MODEL_8b3ef248cf4b4ff4b70a08d706dcb605"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
