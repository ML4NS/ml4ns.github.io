{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6863889",
   "metadata": {},
   "source": [
    "## Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "### Contributors: Antigone Fogel, Nan Fletcher-Lloyd, Anastasia Gailly De Taurines, Iona Biggart, Payam Barnaghi\n",
    "Machine Learning for Neuroscience, <br>Department of Brain Sciences, Faculty of Medicine, <br> Imperial College London\n",
    "\n",
    "**Spring 2026**\n",
    "\n",
    "# Lab 1: Machine Learning for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161ecd",
   "metadata": {},
   "source": [
    "### Types of Machine Learning\n",
    "In this tutorial, we focus on **two core types** of machine learning commonly used for medical applications: \n",
    "- **Supervised learning**, where models learn to predict known outcomes (classification and regression), and \n",
    "- **Unsupervised learning**, where models identify hidden structures in data without known outcomes (e.g. clustering).\n",
    "\n",
    "### How to implement machine learning pipelines in python\n",
    "Throughout this tutorial (and this course) we will use `scikit-learn` (https://scikit-learn.org/stable), the most widely used machine learning library for Python. `scikit-learn` provides built-in implementations for common machine learning algorithms, as well as tools for model selection and data preprocessing.\n",
    "\n",
    "If you are ever in doubt, check out a module or function's documentation at the link above, or by searching for its name.\n",
    "\n",
    "Let's begin by importing scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e7882",
   "metadata": {},
   "source": [
    "And as always, we must remember to import our other dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4b0e5",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "- Explain the difference between features and labels\n",
    "- Fit a simple supervised learning model using scikit-learn\n",
    "- Evaluate model performance on held-out data\n",
    "- Interpret basic classification results and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8253d",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618181b",
   "metadata": {},
   "source": [
    "Supervised learning uses labelled datasets, where each sample consists of input features and a known target label. The goal is to learn a function that maps inputs to outputs. These models can be used to **classify** data into categories or predict continuous values, a task known as **regression**. We will start with classification.\n",
    "\n",
    "Before we begin, we will use the following conventions:\n",
    "- **X**: input features (shape: n_samples × n_features)\n",
    "- **y**: target labels\n",
    "- **sample**: one observation (e.g., one participant)\n",
    "- **feature**: a measured variable (e.g., heart rate, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331de788",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69c4c6",
   "metadata": {},
   "source": [
    "For this example, you are going to use the sci-kit learn **breast cancer dataset**. More on this dataset can be found **[here.](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn datasets come in an unusual format, the below code translates the breast cancer dataset into a DataFrame\n",
    "data = datasets.load_breast_cancer(as_frame=True) \n",
    "data = data.frame\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c74a3c",
   "metadata": {},
   "source": [
    "#### **Data preprocessing**\n",
    "Let's begin by splitting our data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='target') # this derives features as a dataframe (30 features for 569 samples)\n",
    "y = data[['target']] # this derives labels as a dataframe (for 569 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0f399",
   "metadata": {},
   "source": [
    "Let's just do a quick check to make sure everything has loaded as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3255149",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99497d36",
   "metadata": {},
   "source": [
    "*N.B. The labels are binary, with 0 indicating a negative diagnosis and 1 indicating a positive diagnosis of breast cancer, respectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044c5ab",
   "metadata": {},
   "source": [
    "Here, we are going to focus on `mean area` and `mean smoothness`, so the first thing we'll do is extract this data.\n",
    "\n",
    "In the code cell below, update the dataframe `area_and_smoothness` so it contains only the two features listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n",
    "\n",
    "area_and_smoothness = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e6d65",
   "metadata": {},
   "source": [
    "*Note: The datasets provided by scikit-learn are already preprocessed, so common steps such as handling missing values or removing duplicates are not required here. In practice, these steps should always be included.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6475a7",
   "metadata": {},
   "source": [
    "#### **Train-test split**\n",
    "\n",
    "When building a supervised machine learning model, it is essential to evaluate how well it performs on **unseen data**. Testing a model on the same data used for training can lead to **overfitting**, where the model appears to perform perfectly but fails to generalize. To avoid this, we typically hold out a portion of the data as a test set using a method called the **train–test split**.\n",
    "\n",
    "You can read more about the train–test split here:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0f06d",
   "metadata": {},
   "source": [
    "So let's import out train-test split helper from sklearn and apply it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dd6a4",
   "metadata": {},
   "source": [
    "In this case, we split the data so that 80% is used for training and 20% is held out for testing (`test_size=0.20`). \n",
    "\n",
    "We also shuffle our data before splitting to ensure that both the training and test sets are representative of the full dataset. Without shuffling, the split may reflect unintended ordering in the data.\n",
    "\n",
    "Finally, we use the `random_state` parameter to control the shuffling applied to the data before the split to ensure a reproducible output across multiple calls of the function. To apply this, pass any integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(area_and_smoothness, y, test_size = 0.20, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fe23e",
   "metadata": {},
   "source": [
    "Check that the output of this function is what you might expect (number of instances in the training and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd27ca",
   "metadata": {},
   "source": [
    "#### **Feature scaling**\n",
    "\n",
    "Before training our model, we must first **scale our feature data**. Feature scaling is a critical preprocessing step in machine learning, with common approaches including **standardization** and **normalization**.\n",
    "\n",
    "Machine learning algorithms that calculate distance or assume normality are sensitive to **relative scales of features**, meaning that if the data is not scaled, features with a higher value range will dominate the model's decision-making process. Scaling brings all features into comparable ranges and often leads to **faster and more stable model convergence**.\n",
    "\n",
    "Learn more about scaling (when you need to and why, and how to do so) using the links below:\n",
    "\n",
    "- https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35 \n",
    "- https://towardsdatascience.com/feature-scaling-effectively-choose-input-variables-based-on-distributions-3032207c921f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7a8e7",
   "metadata": {},
   "source": [
    "In this scenario, we are going to use the `StandardScaler` tool from sci-kit learn to standardise our data.\n",
    "\n",
    "Learn more about this scaler **[here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)**\n",
    "\n",
    "Another commonly used scaler is the `MinMaxScaler`. The MinMaxScaler normalizes data. Learn more **[here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcee66",
   "metadata": {},
   "source": [
    "First, let's import out scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9654613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a4439",
   "metadata": {},
   "source": [
    "Next, let's implement our standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617898ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(scaler.fit_transform(x_train),\n",
    "                   columns=['mean area','mean smoothness'])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e90aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(scaler.transform(x_test),\n",
    "                   columns=['mean area','mean smoothness'])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0688",
   "metadata": {},
   "source": [
    "Notice how we `.fit_transform` our training data, but only `.transform` our testing data. **In your own words describe why**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e79f6",
   "metadata": {},
   "source": [
    "#### **Model training**\n",
    "\n",
    "Now that we have preprocessed our data, selected our features, split our data into training and testing sets and scaled it, it's time to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029e74d",
   "metadata": {},
   "source": [
    "For this example, we are going to use logistic regression which, despite its name, is a linear classification model. Learn more about logistic regression **[here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)**\n",
    "\n",
    "To use the Logistic Regression classifier, we must first import it using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c84149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a271d",
   "metadata": {},
   "source": [
    "Next, we will use the model we just trained to predict the labels of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aebb4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e5b19",
   "metadata": {},
   "source": [
    "We can now evaluate the quality of our model’s predictions using several different metrics. We begin with the most familiar metric: **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.score(x_test, y_test) \n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5992326",
   "metadata": {},
   "source": [
    "Accuracy tells us the overall proportion of samples that were correctly classified:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "Now, accuracy is not the only metric that can be used to evaluate model performance. \n",
    "\n",
    "Read more about the different metrics that can be used to evaluate model performance including how to calculate them and when to use them **[here](https://towardsdatascience.com/performance-metrics-for-classification-machine-learning-problems-97e7e774a007)**.\n",
    "\n",
    "To compute these metrics, we will use these predicted labels to derive a confusion matrix. A confusion matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). We'll start by importing the confusion matrix helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8679f3",
   "metadata": {},
   "source": [
    "We can then print the confusion matrix using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b4cc9",
   "metadata": {},
   "source": [
    "We can also plot the confusion matrix using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two classes in the list object below to label the confusion matrix with the appropriate groupings.\n",
    "class_names = []\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    # xticklabels=class_names,\n",
    "    # yticklabels=class_names,\n",
    "    cbar=False\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b30af09e",
   "metadata": {},
   "source": [
    "From this confusion matrix, we can then derive further metrics, including precision, recall, and the F1-score:\n",
    "\n",
    "- **Precision** is the ratio of correctly classified positive instances to the total predicted positive classifications. \n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Recall (aka Sensitivity)** is the ratio of correctly classified positive instances to the total positive instances.\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **F1-score** balances precision and recall by taking their harmonic mean.\n",
    "$$\n",
    "\\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9b319",
   "metadata": {},
   "source": [
    "First, let's import our helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7164ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd3d16",
   "metadata": {},
   "source": [
    "Now, let's print all these scores.\n",
    "\n",
    "For precision, recall, and the F1-score, **you should report the average and standard deviation** of these scores. This is because the precision, recall, and F1-score is provided for each class in the dataset.\n",
    "\n",
    "However, particularly when working with multiclass data, it is important to understand the performance of the model for each class. For this, we print a classification report, which tells you the precision, recall, and F1-score for each class and support (weighted by number of instances in each class in comparison to total number of instances for each class, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f22a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, pred_labels)\n",
    "recall = recall_score(y_test, pred_labels)\n",
    "precision = precision_score(y_test, pred_labels)\n",
    "\n",
    "f1_avg = mean(f1_score(y_test, pred_labels, average=None))\n",
    "recall_avg = mean(recall_score(y_test, pred_labels, average=None))\n",
    "precision_avg = mean(precision_score(y_test, pred_labels, average=None))\n",
    "\n",
    "f1_sd = std(f1_score(y_test, pred_labels, average=None))\n",
    "recall_sd = std(recall_score(y_test, pred_labels, average=None))\n",
    "precision_sd = std(precision_score(y_test, pred_labels, average=None))\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1_avg)\n",
    "print('recall_avg\\t',recall_avg)\n",
    "print('precision_avg\\t',precision_avg)\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1_sd)\n",
    "print('recall_sd\\t',recall_sd)\n",
    "print('precision_sd\\t',precision_sd)\n",
    "\n",
    "print('\\n',classification_report(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d2496",
   "metadata": {},
   "source": [
    "Another metric that can be used to evaluate model performance is the **area under the receiver operating characteristic curve (ROC-AUC)**, which plots the true positive rate (recall) against the false positive rate. The AUC ranges in value from 0 to 1. The closer the score to 1, the greater the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e14cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bf050",
   "metadata": {},
   "source": [
    "Repeat the train-test split with a different `random_state`. Do the evaluation metrics change? Why might this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa828471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411bed2",
   "metadata": {},
   "source": [
    "#### **Cross-validation**\n",
    "Even with a train–test split, it is still possible to **overfit** when tuning model parameters, as repeated evaluation can allow information from the test set to leak into the model. In this case, performance metrics may no longer reflect true generalisation to unseen data.\n",
    "\n",
    "A common way to address this is **cross-validation**, which provides a more reliable estimate of model performance without requiring a third validation set.\n",
    "\n",
    "In k-fold cross-validation, the data is divided into k equally sized folds. The model is trained on k − 1 folds and evaluated on the remaining fold. This process is repeated k times, once for each fold, and the final performance is reported as the average across all folds.\n",
    "\n",
    "You can learn more about cross-validation **[here](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a96a4",
   "metadata": {},
   "source": [
    "For this, we must first import the cross_val_predict helper function. This allows us to fit a model to our data and generate cross-validated estimates for each input data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944adf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e26670",
   "metadata": {},
   "source": [
    "To ensure that data standardisation is performed **correctly within each cross-validation fold**, we use something called a **pipeline**. A pipeline learns preprocessing steps (like scaling) only from the training data in each fold and then applies the same transformation to the held-out test data, preventing data leakage.\n",
    "\n",
    "In scikit-learn, this is done using `make_pipeline` (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Pipelines make it easy to chain preprocessing steps and models together during cross-validation (see also: https://scikit-learn.org/stable/modules/compose.html#combining-estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bed83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), \n",
    "                      LogisticRegression(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6867c",
   "metadata": {},
   "source": [
    "And finally, we generate cross-validated estimates for each input data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09af385",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred_labels = cross_val_predict(model, area_and_smoothness, y.values.ravel(), cv=10) # Here we use a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3df144",
   "metadata": {},
   "source": [
    "Now all we need to do is output our results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, cv_pred_labels)\n",
    "\n",
    "cm = confusion_matrix(y, cv_pred_labels)\n",
    "\n",
    "f1 = f1_score(y, cv_pred_labels)\n",
    "recall = recall_score(y, cv_pred_labels)\n",
    "precision = precision_score(y, cv_pred_labels)\n",
    "\n",
    "f1_avg = mean(f1_score(y, cv_pred_labels, average=None))\n",
    "recall_avg = mean(recall_score(y, cv_pred_labels, average=None))\n",
    "precision_avg = mean(precision_score(y, cv_pred_labels, average=None))\n",
    "\n",
    "f1_sd = std(f1_score(y, cv_pred_labels, average=None))\n",
    "recall_sd = std(recall_score(y, cv_pred_labels, average=None))\n",
    "precision_sd = std(precision_score(y, cv_pred_labels, average=None))\n",
    "\n",
    "print('accuracy:\\t', accuracy)\n",
    "print('ROC-AUC\\t\\t',roc_auc_score(y, cv_pred_labels))\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1_avg)\n",
    "print('recall_avg\\t',recall_avg)\n",
    "print('precision_avg\\t',precision_avg)\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1_sd)\n",
    "print('recall_sd\\t',recall_sd)\n",
    "print('precision_sd\\t',precision_sd)\n",
    "\n",
    "print('\\n',classification_report(y, cv_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664cb551",
   "metadata": {},
   "source": [
    "And that's it! You have now learnt all about classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331de788",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69c4c6",
   "metadata": {},
   "source": [
    "For this example, you are going to use the sci-kit learn **diabetes dataset**. In this dataset, the target is a continuous measure of diabetes diseases progression one year after baseline. More on this dataset can be found **[here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_diabetes(as_frame=True, scaled=False)\n",
    "data = data.frame\n",
    "X = data.drop(columns='target') \n",
    "y = data[['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0f399",
   "metadata": {},
   "source": [
    "Let's just do a quick check to make sure everything has loaded as it should be. Start by printing `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318831a1",
   "metadata": {},
   "source": [
    "**QUESTION**: How many samples are in the diabetes dataset? How many features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d58ef",
   "metadata": {},
   "source": [
    "First, let's check the linear correlation between variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54713e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "# visualise the correlation matrix\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, cmap='RdPu', annot=True, fmt='.2f', cbar=True, annot_kws={\"size\": 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c98d9",
   "metadata": {},
   "source": [
    "#### **Train-test split**\n",
    "\n",
    "As we did with classification, let's begin with a simple **train-test split** to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdf523",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c39c7",
   "metadata": {},
   "source": [
    "#### **Feature scaling**\n",
    "In the code cell below, scale your training and testing data using the `StandardScaler()` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8c45c",
   "metadata": {},
   "source": [
    "#### **Model training**\n",
    "\n",
    "For this example, we are going to use linear regression. You can learn more about linear regression **[here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549cb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f880c6",
   "metadata": {},
   "source": [
    "We can now fit our model on the training data and use it to predict the target values for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5550a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27846615",
   "metadata": {},
   "source": [
    "#### **Model evaluation**\n",
    "\n",
    "Finally, let's evaluate the model.\n",
    "\n",
    "To speed things up, we can import metrics from sklearn using the code in the next cell. Learn more about the different metrics (what they measure and when to use them) **[here](https://scikit-learn.org/stable/modules/model_evaluation.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2779dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80631692",
   "metadata": {},
   "source": [
    "We are going to look at three measures:\n",
    "- the **coefficient of determination** or **R²** score represents the proportion of the variance that has been explained by the features of the model. It provides an indication of goodness of fit, with the best possible score being 1.0. More information can be found **[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)**.\n",
    "- the **mean absolute error** or **MAE** computes a risk metric corresponding to the expected value of the absolute error loss. More information can be found **[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)**.\n",
    "- the **mean squared error** or **MSE** computes a risk metric corresponding to the expected value of the squared error or loss. More information can be found **[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea471697",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The R² score of the model is\", np.round(mt.r2_score(y_test,y_pred),2))\n",
    "print(\"The Mean Absolute Error of model is\", np.round(mt.mean_absolute_error(y_test,y_pred),2))\n",
    "print(\"The Mean Squared Error of the model is\" , np.round(mt.mean_squared_error(y_test,y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3268b8",
   "metadata": {},
   "source": [
    "Finally, we examine the **regression coefficients** and **interecept** learned by the model.\n",
    "\n",
    "Each regression coefficient provides two key pieces of information: \n",
    "1. The **direction** of the relationship between a given feature and the outcome value, and\n",
    "2. The **magnitude** of the effect of that feature on the outcome.\n",
    "\n",
    "The intercept prepresenta the expected value of the outcome variable when all features are equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model.coef_.ravel(), index=x_train.columns) # derives the coefficient for each variable\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = model.intercept_ # derives the model intercept\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db4488",
   "metadata": {},
   "source": [
    "The regression equation as calculated by the model would be as follows:\n",
    "\n",
    "Diabetes Progression = intercept + coef(1) x age + coef(2) x sex + coef(3) x bmi + coef(4) x bp + coef(5) x s1 + coef(6) x s2 + coef(7) x s3 + coef(8) x s4 + coef(9) x s5 + coef(10) x s6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177504d2",
   "metadata": {},
   "source": [
    "What happens if you remove feature scaling from your pipeline? How do the coefficients and performance metrics change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c7832f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8253d",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618181b",
   "metadata": {},
   "source": [
    "Unsupervised learning uses machine learning algorithms to analzye unlabelled datasets. Below, we will discuss the most common form of unsupervised learning: **Clustering**. Clustering is the automatic grouping of similar objects into sets.\n",
    "\n",
    "For this example, we are going to use the sci-kit learn **Iris flower dataset** containing a total of 150 samples across 3 classes (species of Iris) for which four features (sepal length, sepal width, petal length, and petal width) were measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris(as_frame=True) \n",
    "data = data.frame\n",
    "X = data.drop(columns='target') \n",
    "y = data[['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280051f9",
   "metadata": {},
   "source": [
    "As before, start by checking how the features and targets are presented in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "256e925f",
   "metadata": {},
   "source": [
    "#### **Feature scaling**\n",
    "\n",
    "Unsupervised learning does not require training and testing splits, so we can move right along to feature scaling.\n",
    "\n",
    "In the cell below, update the code such that `standardized_data` contains your standardised feature values.\n",
    "\n",
    "*N.B. The feature variables are all measured in cm, meaning they share a common scale. In this case, standardizing the data may not have much effect, but it is still good practice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b04426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##\n",
    "standardized_data = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89f4d4",
   "metadata": {},
   "source": [
    "#### **Data visualisation**\n",
    "\n",
    "Before we cluster this dataset, we want to understand the relationship between each variable and every other variable within the dataset. To do this, we use the function `sns.pairplot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735327f",
   "metadata": {},
   "source": [
    "First, we need to concatenate the data (features and labels) into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.concat([standardized_data, y], axis=1)\n",
    "iris.rename(columns = {'target':'Class'}, inplace=True)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4219a37",
   "metadata": {},
   "source": [
    "Next, we plot the pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "g = sns.pairplot(data=iris, hue=\"Class\", height=3)\n",
    "sns.move_legend(g, \"lower center\", bbox_to_anchor=(0.5, -0.035), ncol=3, title='Class', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5aa97",
   "metadata": {},
   "source": [
    "#### **Clustering**\n",
    "\n",
    "For this example, we are going to use **K-Means Clustering**, a distance-based algorithm.\n",
    "\n",
    "K-Means aims to partition the data into **k** clusters (where *k* is chosen in advance). The algorithm works iteratively:\n",
    "\n",
    "1. **Initialize** *k* cluster centers (centroids), typically by selecting starting positions automatically (often at random).\n",
    "2. **Assign** each data point to the cluster with the nearest centroid (based on distance).\n",
    "3. **Update** each centroid by taking the mean of all points assigned to that cluster.\n",
    "\n",
    "Steps 2–3 repeat until the assignments stop changing (or a maximum number of iterations is reached). The final result is a set of cluster labels and cluster centers.\n",
    "\n",
    "*Because K-Means is distance-based, it is usually important to **scale features** before clustering.*\n",
    "\n",
    "Learn more about K-Means Clustering and other types of clustering methods **[here](https://scikit-learn.org/stable/modules/clustering.html#k-means)**\n",
    "\n",
    "Learn more about how to use K-Means Clustering function **[here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99f6ae",
   "metadata": {},
   "source": [
    "Let's start by importing KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e6f2e",
   "metadata": {},
   "source": [
    "Next, we want to find the optimum number of clusters for k-means classification. The KMeans algorithm clusters data by minimizing what is known as the inertia (the within-cluster sum-of-squares). To find the optimum number of clusters, we must first calculate the within-cluster sum-of-squares. We do this using only the features. For this to work, our feature data must be in array form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac182aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_array = standardized_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "\n",
    "for k in range(1, 11): # sets k in the range of 1 - 11\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42) \n",
    "    kmeans.fit(feat_array)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# max_iter is the number of iterations of the k-means algorithm in a single run (number of times the centroid is recomputed)\n",
    "# the n_init is the number of times the k-means algorithm is run with different centroid seeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a750fe96",
   "metadata": {},
   "source": [
    "We then use something known as the elbow method to determine the optimal number of clusters for k-means clustering. \n",
    "\n",
    "Here, we know that there should be three clusters, but otherwise we pick the 'elbow' of the curve as the optimum number of clusters, i.e. where adding another cluster does not allow for much better modelling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1, 11), wcss, color='indigo')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb07f1d",
   "metadata": {},
   "source": [
    "Now we've found the optimum number of clusters, we can implement K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(feat_array) # predict the cluster index of each sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b80b48f0",
   "metadata": {},
   "source": [
    "We can then visualise the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# visualising the clusters \n",
    "plt.scatter(feat_array[y_kmeans == 0, 0], feat_array[y_kmeans == 0, 1], s = 100, c = 'darkviolet', label = 'Cluster 0')\n",
    "plt.scatter(feat_array[y_kmeans == 1, 0], feat_array[y_kmeans == 1, 1], s = 100, c = 'deeppink', label = 'Cluster 1')\n",
    "plt.scatter(feat_array[y_kmeans == 2, 0], feat_array[y_kmeans == 2, 1], s = 100, c = 'orchid', label = 'Cluster 2')\n",
    "\n",
    "# plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')\n",
    "\n",
    "# plots legend outside of the grid, bottom centre of the figure\n",
    "plt.legend(ncol = 4, bbox_to_anchor=(0.9, -0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a30277d",
   "metadata": {},
   "source": [
    "#### **Exlainability**\n",
    "\n",
    "While the scatterplot above provides a good visual overview of how well our clusters separate, it is very important to understand which features are driving this separation. The remainder of this tutorial will therefore focuses on explaining the clustering results. \n",
    "\n",
    "We want to understand how our samples cluster, and the easiest way to do this is to use the mean of their characteristics. We call this process **profiling**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c326fda",
   "metadata": {},
   "source": [
    "For these next visualisations, we need to install and import plotly, an open-source plotting library in python.\n",
    "\n",
    "Learn more here: https://plotly.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user plotly -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eae06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a312ee",
   "metadata": {},
   "source": [
    "Now, we want to visualise the mean of each feature variable by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(feat_array) # fit the model \n",
    "clusters = standardized_data.copy()\n",
    "clusters['Cluster'] = kmeans.labels_ # assigns cluster labels to features\n",
    "polar = clusters.groupby(\"Cluster\").mean().reset_index() # finds the mean of each feature variable by cluster\n",
    "polar = pd.melt(polar, id_vars=[\"Cluster\"])\n",
    "fig = px.line_polar(polar, r=\"value\", theta=\"variable\", color=\"Cluster\", line_close=True, height=400, width=700, color_discrete_sequence=['darkviolet','deeppink','orchid'], template=\"plotly_dark\")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=1.05,\n",
    "    xanchor=\"right\",\n",
    "    x=1.05\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cb77401",
   "metadata": {},
   "source": [
    "It's also useful to be able to visualise how many samples are in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie = clusters.groupby('Cluster').size().reset_index()\n",
    "pie.columns=['Cluster','Value']\n",
    "fig = px.pie(pie, values='Value', names='Cluster', color='Cluster', category_orders={\"Cluster\": [\"0\", \"1\", \"2\"]}, height=400, width=700, color_discrete_sequence=['darkviolet','deeppink','orchid'], template=\"plotly_dark\")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=1.05,\n",
    "    xanchor=\"right\",\n",
    "    x=1.05\n",
    "))\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2bed662",
   "metadata": {},
   "source": [
    "From these plots, we can understand the characteristics of each cluster of irises. \n",
    "\n",
    "For example, if you look at Cluster 1, you can understand that the petal width, petal length, and sepal length of these irises were much smaller than in the other two clusters but that they're sepal width was a lot larger.\n",
    "\n",
    "Clusters 0 and 2 have a similar shape, with Cluster 2 having consistently larger measurements than Cluster 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4426bc04",
   "metadata": {},
   "source": [
    "Finally, you can apply names to your clusters based on these characteristics.\n",
    "\n",
    "- Cluster 0 is \"the small irises\"/\"the common irises\"\n",
    "- Cluster 1 is \"the wide sepal irises\"\n",
    "- Cluster 2 is \"the common irises\"/\"the large irises\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c1da3",
   "metadata": {},
   "source": [
    "What happens if you refit the K-Means model with 5 clusters? Visualise the new cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc9bfe",
   "metadata": {},
   "source": [
    "And that's it! You've learnt the basics of clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd194e07",
   "metadata": {},
   "source": [
    "Now you've finished this tutorial, follow the instructions and complete the assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
