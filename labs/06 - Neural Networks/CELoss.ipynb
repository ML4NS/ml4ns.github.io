{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross entropy loss**\n",
    "\n",
    "The cross-entropy loss (i.e. log loss), calculates the classification error when the corresponding prediction for each class is given as a value between 0 and 1. Cross-entropy loss increases as the predicted probability for the true calass decreases. In other words, predicting a low probability when the actual class label is 1 would result in a higher loss value. A perfect predictiob model would have a log loss of 0.\n",
    "The cross entropy loss for a binary classification is calculated as:\n",
    "\n",
    "Binary Cross Entropy Loss (BCELoss):\n",
    "\n",
    "$$ \n",
    "BCELoss = âˆ’(ð‘¦log(ð‘)+(1âˆ’ð‘¦)log(1âˆ’ð‘))\n",
    "$$\n",
    "\n",
    "for the class number more than two (i.e. C_num>2): \n",
    "\n",
    "The multiclass cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( L \\): The total loss.\n",
    "- \\( N \\): Number of samples.\n",
    "- \\( C \\): Number of classes.\n",
    "- \\( y_{ij} \\): Ground truth indicator (1 if sample \\( i \\) belongs to class \\( j \\), 0 otherwise).\n",
    "- ( \\hat{y}_{ij} \\): Predicted probability for sample \\( i \\) and class \\( j \\), typically output from a softmax function.\n",
    "\n",
    "For a **batch-averaged** version of the loss:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.9012950658798218\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Sample predictions and targets\n",
    "# Predictions from the model per class; here we have 4 classes and each value is the predicted value for that class e.g. 0.95 for class 0, 0.25 for class 1, 0.25 for class 2, and 0.25 for class 3\n",
    "predictions = torch.tensor([[0.95, 0.25, 0.25, 0.25], [0.1, 0.2, 0.3, 0.94]], requires_grad=True)   \n",
    "targets = torch.tensor([0, 3]) # Actual class labels e.g. 0 for class 0, 3 for class 3\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "print(f'Cross-Entropy Loss: {loss.item()}')\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
