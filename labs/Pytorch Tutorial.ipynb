{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94a7449",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch (2026)\n",
    "\n",
    "Iona Biggart, Antigone Fogel, Anastasia Gailly de Taurine, Nan Fletcher-Lloyd, Payam Barnaghi\n",
    "\n",
    "## What is the point of this notebook?\n",
    "\n",
    "The goal of this notebook is to understand **how data flows through a PyTorch machine learning pipeline**.\n",
    "\n",
    "We will:\n",
    "- Turn raw data into **tensors**\n",
    "- Load data in **Dataloaders and batches**\n",
    "- Define a **model** that makes predictions\n",
    "- Measure errors with a **loss function**\n",
    "- Improve the model using **optimisation**\n",
    "- Evaluate performance on **unseen data**\n",
    "\n",
    "The focus is on **understanding the structure**, not on building a complex model.\n",
    "\n",
    "> By the end, you should be able to read, write, and modify a basic PyTorch training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ef73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Utilities for datasets and batching\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Vision datasets (MNIST is included here)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# additional\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd401d46",
   "metadata": {},
   "source": [
    "## Tensors: The Building Blocks of PyTorch\n",
    "\n",
    "Tensors are the main data structure used in PyTorch.  \n",
    "They store numbers and have a **shape**, **data type**, and **device**.\n",
    "\n",
    "Everything in PyTorch — inputs, outputs, and model parameters — is a tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5d92e",
   "metadata": {},
   "source": [
    "### 1. Creating tensors from Python lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar (0D tensor)\n",
    "a = torch.tensor(3)\n",
    "\n",
    "# Vector (1D tensor)\n",
    "b = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Matrix (2D tensor)\n",
    "c = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "a, b, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4971939",
   "metadata": {},
   "source": [
    "### 2. Checking tensor properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", c.shape)\n",
    "print(\"Data type:\", c.dtype)\n",
    "print(\"Device:\", c.device) # CPU or GPU \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460979c8",
   "metadata": {},
   "source": [
    "### 3. Creating tensors with built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros(3, 4) # 3 rows, 4 columns of 0s\n",
    "ones = torch.ones(2, 2) # 2 rows, 2 columns of 1s\n",
    "random = torch.rand(2, 3) # 2 rows, 3 columns of random values between 0 and 1\n",
    "\n",
    "zeros, ones, random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc645d52",
   "metadata": {},
   "source": [
    "### 4. Basic tensor operations\n",
    "#### 4a. Element-wise addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(x + y) # Element-wise addition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05526c7",
   "metadata": {},
   "source": [
    "#### 4b. Matrix multiplication \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input matrix (e.g. a batch of 2 samples with 3 features)\n",
    "X = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Weight matrix (maps 3 input features to 2 outputs)\n",
    "W = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]])\n",
    "\n",
    "# Matrix multiplication\n",
    "Y = X @ W\n",
    "\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11b406",
   "metadata": {},
   "source": [
    "### 6. Moving tensors to GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU information\n",
    "num_cpus = os.cpu_count()\n",
    "print(f\"Number of CPUs available: {num_cpus}\")\n",
    "\n",
    "# GPU information\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "if num_gpus > 0:\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002b8db",
   "metadata": {},
   "source": [
    "### Side note: GPU vs CUDA (What’s the Difference?)\n",
    "\n",
    "These two terms are often confused, but they are **not the same thing**.\n",
    "\n",
    "### GPU (Graphics Processing Unit)\n",
    "- A **piece of hardware**\n",
    "- Designed to perform many calculations in parallel\n",
    "- Used to speed up deep learning computations\n",
    "- Can come from different vendors (NVIDIA, AMD, Apple)\n",
    "\n",
    "### CUDA\n",
    "- A **software platform** created by NVIDIA\n",
    "- Allows programs (like PyTorch) to run code on **NVIDIA GPUs**\n",
    "- Provides tools, drivers, and libraries for GPU acceleration\n",
    "- Only works with **NVIDIA GPUs**\n",
    "\n",
    "### How this relates to PyTorch\n",
    "- PyTorch can run on:\n",
    "  - CPU\n",
    "  - NVIDIA GPUs (via CUDA)\n",
    "  - Apple GPUs (via Metal / MPS)\n",
    "- When you use `device=\"cuda\"`, you are using **CUDA on an NVIDIA GPU**\n",
    "\n",
    "### Key takeaway\n",
    "> *GPU is the hardware. CUDA is NVIDIA’s way of using it.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e050793",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset\n",
    "\n",
    "We now load a **real dataset** so we can train and evaluate a neural network.\n",
    "\n",
    "**MNIST** is a classic dataset of handwritten digits:\n",
    "- Each image is a **28 × 28 grayscale image**\n",
    "- Each image shows a digit from **0 to 9**\n",
    "- It is commonly used to learn and test deep learning pipelines\n",
    "\n",
    "### Why we need preprocessing (transforms)\n",
    "\n",
    "Raw images cannot be used directly by PyTorch models.\n",
    "We apply **transforms** to prepare the data:\n",
    "\n",
    "- Convert images into **PyTorch tensors**\n",
    "- Scale pixel values to a **standard numerical range**\n",
    "- Ensure the data has a consistent format for training\n",
    "\n",
    "### Train vs Validation vs Test data\n",
    "\n",
    "- **Training data** is used to teach the model\n",
    "- **Validation set**  \n",
    "  - Monitor performance\n",
    "  - Detect overfitting\n",
    "  - Tune hyperparameters\n",
    "\n",
    "- **Test set**  \n",
    "  - Used only once at the very end  \n",
    "  - Gives an unbiased estimate of real-world performance\n",
    "\n",
    "> The test set should never influence training decisions!!!\n",
    "\n",
    "- This separation helps measure how well the model generalises\n",
    "\n",
    "In the next cell, we:\n",
    "- Define the preprocessing steps\n",
    "- Download the MNIST dataset\n",
    "- Create training and test datasets ready for batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322484ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),              # Convert image → PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalise values to ~[-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2f3d",
   "metadata": {},
   "source": [
    "## Creating DataLoaders\n",
    "\n",
    "Datasets store the data, but models are trained using **batches of data**.  \n",
    "A **DataLoader** controls how data is grouped, ordered, and fed to the model.\n",
    "\n",
    "### What a DataLoader does\n",
    "- Splits data into **mini-batches**\n",
    "- Controls **shuffling** of samples\n",
    "- Efficiently loads data during training and evaluation\n",
    "\n",
    "### Shuffling: why it matters\n",
    "\n",
    "- **Training (`shuffle=True`)**\n",
    "  - Randomises the order of samples each epoch\n",
    "  - Prevents the model from learning order-specific patterns\n",
    "  - Improves generalisation\n",
    "\n",
    "- **Validation & Test (`shuffle=False`)**\n",
    "  - Keeps data order fixed\n",
    "  - Ensures reproducible and comparable evaluation\n",
    "  - Reflects real-world inference conditions\n",
    "\n",
    "### Batch size\n",
    "\n",
    "- `batch_size` controls how many samples the model sees at once\n",
    "- Smaller batches → noisier updates, lower memory usage\n",
    "- Larger batches → smoother updates, higher memory usage\n",
    "\n",
    "In the next cell, we create DataLoaders for:\n",
    "- Training\n",
    "- Validation\n",
    "- Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebe5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise some training data\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4bb2a",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "We now define a **neural network** that maps input images to digit predictions.\n",
    "\n",
    "### What this model does\n",
    "- Takes a **28 × 28 image** as input\n",
    "- Flattens it into a vector\n",
    "- Learns intermediate representations\n",
    "- Outputs **10 scores**, one for each digit (0–9)\n",
    "\n",
    "### Key ideas\n",
    "- Models are built by subclassing `nn.Module`\n",
    "- Layers define **what can be learned**\n",
    "- The `forward()` method defines **how data flows**\n",
    "- Inputs and outputs are **tensors**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class defines a simple feedforward neural network for image classification.\n",
    "\n",
    "    - Takes 28×28 images as input (MNIST)\n",
    "    - Flattens each image into a vector of length 784\n",
    "    - Uses fully connected (Linear) layers with a ReLU activation\n",
    "    - Outputs 10 scores, one for each digit class (0–9)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self): # Define layers and components \n",
    "        super().__init__() # Initialize the parent class (nn.Module)\n",
    "\n",
    "        # Flatten 28x28 image → vector of length 784\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) # Hidden layer with 128 neurons, input size 784\n",
    "        self.relu = nn.ReLU() # Activation function, introduces non-linearity\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes (digits 0–9), output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define how data flows through the network\n",
    "        x = self.flatten(x)  # Flatten the input\n",
    "        x = self.fc1(x)  # Apply first fully connected layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNeuralNetwork().to(device) #Define the model and move to device. All parameters are now on the device.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss: loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam is a commonly used adaptive optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97f61d",
   "metadata": {},
   "source": [
    "## Training and Validation Loop\n",
    "\n",
    "We now train the model and evaluate it on **validation data** after each epoch.\n",
    "\n",
    "### What happens during training\n",
    "- The model sees batches of training data\n",
    "- Predictions are compared to true labels\n",
    "- Errors (loss) are backpropagated\n",
    "- Model weights are updated\n",
    "\n",
    "### What happens during validation\n",
    "- The model is switched to **evaluation mode**\n",
    "- No gradients are computed\n",
    "- Performance is measured on unseen data\n",
    "- We monitor generalisation, not learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d47cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 5 # Number of times to iterate over the entire training dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0 # Initialise running loss for the epoch\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to CPU/GPU\n",
    "        images = images.to(device) # Important: move images to the same device as the model\n",
    "        labels = labels.to(device) # Move labels to device\n",
    "\n",
    "        # ---- Forward pass ----\n",
    "        outputs = model(images) # Get model predictions\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "\n",
    "        # ---- Backward pass ----\n",
    "        optimizer.zero_grad()  # Reset gradients (remove old gradients from previous step)\n",
    "        loss.backward()        # Compute new gradients \n",
    "        optimizer.step()       # Update weights\n",
    "        # These three steps ensure each weight update is based only on the current batch’s loss.\n",
    "\n",
    "        running_loss += loss.item() # Accumulate loss over batches\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader) # Average loss for the epoch\n",
    "    train_losses.append(avg_loss) # Store training loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "     # ---- Validation ----\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() # accumulate validation loss over batches\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader) # average validation loss per epoch\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d84674",
   "metadata": {},
   "source": [
    "## Identifying Overfitting\n",
    "\n",
    "- Training loss usually **decreases steadily**\n",
    "- Validation loss decreases at first, then may **stop improving**\n",
    "- If validation loss increases while training loss keeps decreasing:\n",
    "  → the model is **overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0101da",
   "metadata": {},
   "source": [
    "## Final Evaluation on the Test Set\n",
    "\n",
    "After training and validation, we evaluate the model **once** on the test dataset.\n",
    "\n",
    "### Purpose of the test set\n",
    "- Measures **true performance** on unseen data\n",
    "- Must not influence training or model choices\n",
    "- Used only after all training decisions are final\n",
    "\n",
    "### What happens during testing\n",
    "- The model is set to **evaluation mode**\n",
    "- Gradient computation is disabled\n",
    "- Predictions are compared to true labels\n",
    "- Accuracy is computed over the entire test set\n",
    "\n",
    "This gives an unbiased estimate of how the model would perform in the real world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    " \n",
    "        total += labels.size(0) # Total number of labels\n",
    "        correct += (predictions == labels).sum().item() # Count correct predictions\n",
    "\n",
    "accuracy = 100 * correct / total \n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d48486",
   "metadata": {},
   "source": [
    "## Example 2: Working with Tabular Data (CSV)\n",
    "\n",
    "So far, we used image data.  \n",
    "Now we show the **same PyTorch pipeline** using data from a **CSV file**, which is very common in science and industry.\n",
    "\n",
    "In this example:\n",
    "- Rows = data samples\n",
    "- Columns = features\n",
    "- One column = target (label)\n",
    "\n",
    "The goal is to show that **PyTorch works the same way regardless of data type**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9219e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame   # includes features + target\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=\"target\").values \n",
    "y = df[\"target\"].values # 3 flower specicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a79e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tells PyTorch how to access our data\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx] # return features and label for given index\n",
    "\n",
    "dataset = IrisDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57dcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size # Remaining samples go to test set\n",
    "\n",
    "generator = torch.Generator().manual_seed(42) # For reproducibility\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f35635",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e687dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class defines a simple Multi-Layer Perceptron (MLP).\n",
    "\n",
    "- Takes input feature vectors as input\n",
    "- Uses fully connected (Linear) layers with ReLU activations\n",
    "- Learns non-linear relationships in the data\n",
    "- Outputs 3 scores, one for each Iris class\n",
    "\"\"\"\n",
    "\n",
    "class MLP(nn.Module): \n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)  # 3 Iris classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP(input_dim=X.shape[1]) # Input dimension based on features\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a05248",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69feb4ba",
   "metadata": {},
   "source": [
    "## PyTorch Beyond Scratch Models: Using Pretrained LLMs\n",
    "\n",
    "So far, we built neural networks **from scratch** using PyTorch.\n",
    "\n",
    "In practice, many modern AI systems use:\n",
    "- **Large pretrained models**\n",
    "- Built using **PyTorch**\n",
    "- Loaded with high-level libraries like Hugging Face or Ollama\n",
    "\n",
    "The key idea:\n",
    "> PyTorch is the engine under the hood — these libraries provide convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8084d78",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers\n",
    "\n",
    "Hugging Face provides access to pretrained models for:\n",
    "- Text generation\n",
    "- Classification\n",
    "- Translation\n",
    "- Question answering\n",
    "\n",
    "Most Hugging Face models are implemented in **PyTorch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022f85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml4ns_pure/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32  # CPU-safe\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbff620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is dementia?\n",
      "Dementia is a syndrome that describes a group of symptoms, including memory loss, confusion and social withdrawal. The symptoms are caused by brain damage from Alzheimer’s, Parkinson’s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is dementia?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=40,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3c9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ns_pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
